<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  Python实现人工神经网络(1) - YaoZh1918
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="YaoZh1918" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:yaozh1918.github.io ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; YaoZh1918</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
       
       <li><a href="index.html">HOME</a></li>
    <li><a href="archives.html">Archives</a></li>
    <li><a href="about.html">ABOUT</a></li>

    <li><label>Categories</label></li>

        
            <li><a href="miscellaneous.html">Miscellaneous</a></li>
        
            <li><a href="algorithm.html">Algorithm</a></li>
        
            <li><a href="python.html">Python</a></li>
        
            <li><a href="linux.html">Linux</a></li>
        
            <li><a href="Math.html">Math</a></li>
        
            <li><a href="ml.html">Machine Learning</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
  $(function(){
    $('#menu_item_index').addClass('is_active');
  });
</script>
<div class="row">
  <div class="large-8 medium-8 columns">
      <div class="markdown-body article-wrap">
       <div class="article">
          
          <h1>Python实现人工神经网络(1)</h1>
     
        <div class="read-more clearfix">
          <span class="date">2017/7/14</span>

          <span>posted in&nbsp;</span> 
          
              <span class="posted-in"><a href='ml.html'>Machine Learning</a></span>
           
         
          <span class="comments">
            

            
              &nbsp;<a  class="ds-thread-count" data-thread-key="15000325978554.html" data-count-type="comments" href="15000325978554.html#ds-thread"></a>
            
          </span>

        </div>
      </div><!-- article -->

      <div class="article-content">
      <p>最近刚听完<a href="https://cs231n.github.io">cs231n</a>，突然想自己实现一下神经网络，故开这么一个坑。repo在<a href="https://github.com/YaoZh1918/PyNeuralNet">这里</a>。基本想法就是仿照Tensorflow和Theano实现一个计算图(Computation Graph)，可以自动传梯度。自己写有什么好处呢，其一是开心XD，另外就是可以加深理解，比方说为什么Deconv叫反卷积，为什么有一群人在争论Deconv是不是个好名字，自己写一下Conv和Deconv的forward和backward操作就明白了。</p>

<p>这个系列的第一篇文章就实现下最简单的计算图，都是标量运算，主要理解下BackPropagation的思想。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_0">BP与Chain Rule</h2>

<p>这里简单讲一下BP的思想，毕竟已经有很多专门介绍的文章了。</p>

<p>考虑一个简单的模型(来源cs231n)：神经网络中的某一个神经元，接受两个输入\(x,y\)，在其内部经过处理，变成\(z=f(x,y)\)，其输出\(z\)再经过若干神经元，变成了网络的最终输出（通常是损失函数的值）\(f_1\circ f_2 \circ\cdots \circ f_k(z, ...)=g(z, ...)=L\)，假设我们已经得到了损失关于\(z\)的偏导\(\frac{\partial L}{\partial z}\)（可以称为上游梯度，upstream gradient，因为也是其他上游神经元传来的），那计算\(\frac{\partial L}{\partial x}\)与\(\frac{\partial L}{\partial y}\)可直接通过链式法则求得：</p>

<p>\[\frac{\partial L}{\partial x}=\frac{\partial L}{\partial z} \frac{\partial z}{\partial x}, \quad <br/>
\frac{\partial L}{\partial y}=\frac{\partial L}{\partial z} \frac{\partial z}{\partial y},\]</p>

<p>其中，\(\frac{\partial z}{\partial x}\)与\(\frac{\partial z}{\partial y}\)可以看作是local gradients，在进行前向传播时就可以算好，缓存在神经元中，这也是为什么会说反向传播需要前向时2倍存储与计算量。</p>

<p>现在我们就得到了这样一个递归的形式，算出来的\(\frac{\partial L}{\partial x}\)与\(\frac{\partial L}{\partial y}\)可以继续反向传给之前的神经元，起始状态也很简单：\(\frac{\partial L}{\partial L}=1\)，这样很方便就可以算出整个网络参数的梯度。</p>

<p><img src="http://o6jlkx4pl.bkt.clouddn.com/15000121481274.jpg" alt=""/></p>

<h2 id="toc_1">标量计算图</h2>

<p>（代码在<a href="https://github.com/YaoZh1918/PyNeuralNet/blob/master/scalar_graph_v1.ipynb">notebook</a>中）</p>

<p>例子就用cs231n中的：</p>

<p>\[f(w,x) = \frac{1}{1 + \exp[-(w_1x_1 + w_2x_2 + b)]}.\]</p>

<p>针对这个问题，我们只需要实现4个运算：加、乘、取逆、指数。也就是说，我们的计算图中只需要这4种运算结点，外加一类输入结点。这些结点最少也要有两个方法：<code>forward()</code>与<code>backward()</code>：</p>

<pre><code>import numpy as np
import abc


class Node(abc.ABC):
    
    @abc.abstractmethod
    def forward(self):
        &quot;&quot;&quot;Feed Forward&quot;&quot;&quot;
    
    @abc.abstractmethod
    def backward(self, dout):
        &quot;&quot;&quot;Back Propagate
        Inputs:
            dout: upstream gradient
        &quot;&quot;&quot;

    @property
    def grads(self):
        return self._grads
</code></pre>

<p>最简单的，Variable类型的结点，可以充当输入、常量（不可更新）或参数（可更新），<code>forward()</code>只需返回当前值，<code>backward()</code>也只是将上流梯度作为自己的梯度即可，不需要继续后向传播，即后向传播递归过程的终止状态。</p>

<pre><code>class Variable(Node):
    
    def __init__(self, val):
        self._v = v
    
    def forward(self):
        return self._v
    
    def backward(self, dout):
        self._grads = dout
</code></pre>

<p>接下来两个二元运算，都接受两个<code>Node</code>，在<code>forward()</code>时，递归调用这两个的<code>forward()</code>方法，获取他们的值，再进行相应计算并返回；<code>backward()</code>时，利用local和upstream梯度算出当前梯度，并向后传播。这里我采用的方法是前向时计算local gradient而不缓存原始数据。</p>

<pre><code>class Add(Node):
    
    def __init__(self, a, b):
        self._a = a
        self._b = b
    
    def forward(self):
        v_a = self._a.forward()
        v_b = self._b.forward()
        # self._local_da = 1.
        # self._local_db = 1.
        return v_a + v_b
    
    def backward(self, dout):
        self._grads = [dout, dout]
        self._a.backward(dout)
        self._b.backward(dout)


class Mul(Node):
    
    def __init__(self, a, b):
        self._a = a
        self._b = b
    
    def forward(self):
        v_a = self._a.forward()
        v_b = self._b.forward()
        self._local_da = v_b
        self._local_db = v_a
        return v_a * v_b
    
    def backward(self, dout):
        da = self._local_da * dout
        db = self._local_db * dout
        self._grads = [da, db]
        self._a.backward(da)
        self._b.backward(db)
</code></pre>

<p>最后两个一元运算也类似：</p>

<pre><code>class Inv(Node):
    
    def __init__(self, a):
        self._a = a
    
    def forward(self):
        val = self._a.forward()
        self._local_grads = - 1. / val**2
        return 1. / val
    
    def backward(self, dout):
        self._grads = self._local_grads * dout
        self._a.backward(self._grads)
        

class Exp(Node):
    
    def __init__(self, a):
        self._a = a
    
    def forward(self):
        val = self._a.forward()
        self._local_grads = np.exp(val)
        return self._local_grads
    
    def backward(self, dout):
        self._grads = self._local_grads * dout
        self._a.backward(self._grads)
</code></pre>

<p>最后我们做下测试：</p>

<pre><code class="language-python3"># Init variables
w1, w2, x1, x2, b = [Variable(float(i)) for i in [2, -3, -1, -2, -3]]

# build graph
logit = Add(Add(Mul(w1, x1), Mul(w2, x2)), b)
f = Inv(Add(Variable(1), Exp(Mul(logit, Variable(-1)))))

# Eval
print(&#39;Values: \n&#39;, logit.forward(), f.forward())
# BP
f.backward(1.0)
print(&#39;Gradients: &#39;)
print(&#39;, &#39;.join(&#39;{:.2f}&#39;.format(v.grads) for v in [w1, w2, x1, x2, b]))
</code></pre>

<blockquote>
<p>Values: <br/>
 1.0 0.73105857863<br/>
Gradients: <br/>
-0.20, -0.39, 0.39, -0.59, 0.20</p>
</blockquote>

<p>对比下正确答案，完美！</p>

<p><img src="http://o6jlkx4pl.bkt.clouddn.com/15000149840268.jpg" alt=""/></p>

<h2 id="toc_2">参数共享</h2>

<p>上面通过递归的方式调用，稍微会有点小问题，一是递归过深python会报错（虽然我们这里不太可能发生），另一点就是参数共享时会出错，考虑这样的一个问题：<br/>
\[f = wx_1 + wx_2\]</p>

<pre><code>w, x1, x2 = [Variable(float(i)) for i in [5, 1, 2]]
f = Add(Mul(w, x1), Mul(w, x2))
print(&#39;Value: \n&#39;, f.forward())
f.backward(1.)
print(&#39;Gradients: \n&#39;, [v.grads for v in [w, x1, x2]])
</code></pre>

<p>输出：</p>

<blockquote>
<p>Value: <br/>
 15.0<br/>
Gradients: <br/>
 [2.0, 5.0, 5.0]</p>
</blockquote>

<p>很明显，关于w的梯度算错了，这是因为w接受自上游的梯度应该相加。具体来说，给定一个计算图，它肯定是有向无环图，在前向传播时，按照拓扑排序来算，后向时则反着来，在未遍历到某个节点前，该结点都只累计上游梯度，不计算，遍历到了再计算，并传播。</p>

<p>上面的代码我们可以稍作修改，主要两点改动：<br/>
1. 维护一个列表，记录结点的创建顺序，即拓扑排序；<br/>
2. 新增propagate方法累积梯度，不传播</p>

<p>这里只改了两个二元运算，完整的代码在<a href="https://github.com/YaoZh1918/PyNeuralNet/blob/master/scalar_graph_v2.ipynb">github</a>中。</p>

<pre><code>import numpy as np
import abc


topo_list = []  # Topological sort

class Node(abc.ABC):
    
    def __init__(self):
        topo_list.append(self)
        self._dout = 0.0
    
    @abc.abstractmethod
    def forward(self):
        &quot;&quot;&quot;Feed Forward&quot;&quot;&quot;
    
    @abc.abstractmethod
    def backward(self):
        &quot;&quot;&quot;Back Propagate&quot;&quot;&quot;
    
    def as_terminal(self):
        &quot;&quot;&quot;Let the node be the terminal &quot;&quot;&quot;
        self._dout = 1.
        return self
    
    def propagte(self, dout):
        &quot;&quot;&quot;Aggregate upstream gradients.&quot;&quot;&quot;
        self._dout += dout

    @property
    def grads(self):
        return self._grads


class Variable(Node):
    
    def __init__(self, val):
        super().__init__()
        self._v = val
    
    def forward(self):
        return self._v
    
    def backward(self):
        self._grads = self._dout

        
class Add(Node):
    
    def __init__(self, a, b):
        super().__init__()
        self._a = a
        self._b = b
    
    def forward(self):
        v_a = self._a.forward()
        v_b = self._b.forward()
        return v_a + v_b
    
    def backward(self):
        dout = self._dout
        self._grads = [dout, dout]
        self._a.propagte(dout)
        self._b.propagte(dout)


class Mul(Node):
    
    def __init__(self, a, b):
        super().__init__()
        self._a = a
        self._b = b
    
    def forward(self):
        v_a = self._a.forward()
        v_b = self._b.forward()
        self._local_da = v_b
        self._local_db = v_a
        return v_a * v_b
    
    def backward(self):
        dout = self._dout
        da = self._local_da * dout
        db = self._local_db * dout
        self._grads = [da, db]
        self._a.propagte(da)
        self._b.propagte(db)
        
</code></pre>

<p>测试:</p>

<pre><code>topo_list = []

w, x1, x2 = [Variable(float(i)) for i in [5, 1, 2]]
f = Add(Mul(w, x1), Mul(w, x2))
print(&#39;Value: \n&#39;, f.forward())

f.as_terminal()
for v in reversed(topo_list):
    v.backward()
print(&#39;Gradients: \n&#39;, [v.grads for v in [w, x1, x2]])
</code></pre>

<blockquote>
<p>Value: <br/>
 15.0<br/>
Gradients: <br/>
 [3.0, 5.0, 5.0]</p>
</blockquote>

<p>完全正确！要注意，因为在实例化时我们把上流梯度设置为了0，在反向传播前，要通过<code>.as_terminal()</code>先将最后一个结点的上流梯度设置为1（即\(\frac{\partial L}{\partial L}=1\))。</p>

<p>不过现在代码还是有点丑陋，功能也很欠缺，那么在下一篇文章中，我们将实现基本的矩阵运算的结点，同时要支持batch，再引入一个类似Tensorflow的Graph来管理计算图。</p>


    

      </div>

      <div class="row">
        <div class="large-6 columns">
        <p class="text-left" style="padding:15px 0px;">
      
        </p>
        </div>
        <div class="large-6 columns">
      <p class="text-right" style="padding:15px 0px;">
      
          <a  href="14836189296241.html" 
          title="Next Post: 利用Ngrok实现内网穿透">利用Ngrok实现内网穿透 &raquo;</a>
      
      </p>
        </div>
      </div>
      <div class="comments-wrap">
        <div class="share-comments">
          

          

          
              <div class="ds-thread" data-thread-key="15000325978554.html" data-url="http://yaozh1918.github.io/15000325978554.html" data-title="Python实现人工神经网络(1)"></div>
          
        </div>
      </div>
    </div><!-- article-wrap -->
  </div><!-- large 8 -->




 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="asset/icon.jpg" /></div>
            
                <h1>YaoZh1918</h1>
                <div class="site-des">I'm DataMan!</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/YaoZh1918" title="GitHub">GitHub</a>

  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="miscellaneous.html"><strong>Miscellaneous</strong></a>
        
            <a href="algorithm.html"><strong>Algorithm</strong></a>
        
            <a href="python.html"><strong>Python</strong></a>
        
            <a href="linux.html"><strong>Linux</strong></a>
        
            <a href="Math.html"><strong>Math</strong></a>
        
            <a href="ml.html"><strong>Machine Learning</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15000325978554.html">Python实现人工神经网络(1)</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14836189296241.html">利用Ngrok实现内网穿透</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14791104542390.html">A Smooth Thresholding Function</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14785830863216.html">Ubuntu上安装Transmission Web界面</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14776396449421.html">设置Server开机启动ssh服务</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  


<script type="text/javascript">
var duoshuoQuery = {short_name:'yaozh1918'};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
