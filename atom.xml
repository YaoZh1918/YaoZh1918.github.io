<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[YaoZh1918]]></title>
  <link href="http://yaozh1918.github.io/atom.xml" rel="self"/>
  <link href="http://yaozh1918.github.io/"/>
  <updated>2017-07-19T22:43:56+08:00</updated>
  <id>http://yaozh1918.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[Python实现人工神经网络(2)]]></title>
    <link href="http://yaozh1918.github.io/15004749480094.html"/>
    <updated>2017-07-19T22:35:48+08:00</updated>
    <id>http://yaozh1918.github.io/15004749480094.html</id>
    <content type="html"><![CDATA[
<p>在上一篇<a href="https://yaozh1918.github.io/15000325978554.html">Python实现人工神经网络(1)</a>文章中，我们体验了计算图与BP的思想，不过当时的结点只能处理标量计算，今天，我们实现下能处理基本矩阵运算的计算图，并用它实现一个线性回归。库代码见<a href="https://github.com/YaoZh1918/PyNeuralNet/tree/master/neuralnet_v1">repo</a>。</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Graph</a>
</li>
<li>
<a href="#toc_1">Node</a>
<ul>
<li>
<a href="#toc_2">Input与Variable</a>
</li>
<li>
<a href="#toc_3">Op运算符</a>
<ul>
<li>
<a href="#toc_4">矩阵乘法Matmul</a>
</li>
<li>
<a href="#toc_5">需要Broadcasting的运算</a>
</li>
<li>
<a href="#toc_6">Reduce</a>
</li>
<li>
<a href="#toc_7">其他运算符</a>
</li>
</ul>
</li>
<li>
<a href="#toc_8">重载运算符</a>
</li>
</ul>
</li>
<li>
<a href="#toc_9">实战：线性回归</a>
<ul>
<li>
<a href="#toc_10">生成数据</a>
</li>
<li>
<a href="#toc_11">定义计算图</a>
</li>
<li>
<a href="#toc_12">优化</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">Graph</h2>

<p>上次我们用一个列表维护了计算图的拓扑排序，这次我们委托给一个专门的Graph对象，类似于Tensorflow的Graph和Session的结合。利用Graph管理结点，以后也方便拓展功能。</p>

<p>这里讲一下主要的几个函数，完整的代码可见repo。</p>

<p>首先依赖的模块有：</p>

<pre><code>import networkx as nx
from matplotlib import pyplot as plt
from collections import Counter
import contextlib
</code></pre>

<p>可以猜到，我们实际上用一个<code>networkx.DiGraph</code>有向图对象来协助我们管理我们的计算图，这样我们还可以可视化我们的计算图：</p>

<pre><code>class Graph:
    &quot;&quot;&quot;Computation Graph&quot;&quot;&quot;

    def __init__(self):
        self._g = nx.DiGraph()
        self._name_counter = Counter()
        self.feed_dict = None
    
    def add(self, ins, node):
        &quot;&quot;&quot;
        Add the new node and edges to the graph.
        :param ins: a list of nodes
        :param node: the new node
        &quot;&quot;&quot;
        self._check(ins, node.name)
        self._g.add_node(node, name=node.name)
        for in_node in ins:
            self._g.add_edge(in_node, node)
    
    # ... other methods ...
</code></pre>

<p><code>__init__</code>中，我们实例化了一个<code>DiGraph</code>，添加结点的操作<code>add</code>实际上就是添加到这个有向图中；<code>Counter</code>对象是用来管理结点名字的，不是重点，就跳过啦；<code>feed_dict</code>是为计算图提供输入的，类似tensorflow中的feed_dict，不过用法稍有不同，后面会看到。</p>

<p>接下来是前向和后向传播的两个函数：</p>

<pre><code>class Graph:
    def forward(self):
        &quot;&quot;&quot;forward pass through entire graph&quot;&quot;&quot;
        for node in self.topological():
            node.eval()

    def backward(self, start_from=None):
        &quot;&quot;&quot;Backward Pass (BackPropagation)&quot;&quot;&quot;
        topo_order = self.topological()
        if start_from is None:
            start_from = topo_order[-1]
        start_from._dout += 1.0
        for node in reversed(topo_order):
            node.propagate()
    
    # ...
</code></pre>

<p><code>forward</code>即前向传播，主要就是按照拓扑排序计算了每个结点的值；<code>backward</code>后向传播，按照拓扑排序逆向传播梯度，有一个参数start_from，即梯度从哪一个结点开始传播，将其上游梯度设为<code>1.0</code>，原因在上一篇文章中也解释过了，默认是最后一个结点。现在只是一个粗略的版本，在后续的文章中，我们会稍微优化这两个方法。</p>

<p>接下来是两个非常关键的上下文管理器：</p>

<pre><code>class Graph:
    @contextlib.contextmanager
    def as_default(self):
        &quot;&quot;&quot;Replace the default graph with the current graph.&quot;&quot;&quot;
        backup_g = DEFAULTS[&#39;graph&#39;]
        DEFAULTS[&#39;graph&#39;] = self
        try:
            yield self
        except Exception:
            raise
        finally:
            DEFAULTS[&#39;graph&#39;] = backup_g

    @contextlib.contextmanager
    def one_pass(self, feed_dict=None):
        &quot;&quot;&quot;
        context manager
        :param feed_dict: a dict whose keys are Nodes.
        &quot;&quot;&quot;
        self.feed_dict = feed_dict
        try:
            yield
        except Exception:
            raise
        finally:
            self.feed_dict = None
            self.reset_nodes()
    
    # ...

DEFAULTS = {&#39;graph&#39;: Graph()}

def get_default_graph():
    &quot;&quot;&quot;
    Get default(current) graph.
    &quot;&quot;&quot;
    return DEFAULTS[&#39;graph&#39;]
</code></pre>

<p>首先是<code>as_default</code>，看名字就知道是仿照tensorflow的XD，用法也基本一致。引入这个可以简化代码，新建结点<code>Node</code>对象时，不需要显式传入<code>Graph</code>，结点会自己调用<code>get_default_graph</code>方法，这样，直接一个<code>with</code>，把构建代码写下面就好。</p>

<p>另一个<code>one_pass</code>是在一轮迭代中用的，可以传入<code>feed_dict</code>（即mini batch），获取数据，并最后重置结点（清空缓存和梯度）。在后面的实战环节，可以看到这两个上下文管理器的具体用法。</p>

<p>还有一些其他的方法，比如画图、返回所有结点、返回所有可更新结点，不是重点，就先跳过了。整个文件的结构大概就是这样：</p>

<pre><code>&quot;&quot;&quot;graph.py&quot;&quot;&quot;

DEFAULTS = {&#39;graph&#39;: None}
__all__ = [&#39;Graph&#39;, &#39;get_default_graph&#39;]

class Graph:
    &quot;&quot;&quot;Computation Graph&quot;&quot;&quot;
    # ...        

DEFAULTS = {&#39;graph&#39;: Graph()}

def get_default_graph():
    &quot;&quot;&quot;
    Get default(current) graph.
    &quot;&quot;&quot;
    return DEFAULTS[&#39;graph&#39;]
</code></pre>

<h2 id="toc_1">Node</h2>

<p><code>Node</code>实例化时，只需将自己加入到对应的计算图中就好，<code>Graph</code>对象用<code>get_default_graph</code>获得。</p>

<pre><code>import numpy as np
import abc
from neuralnet_v1 import get_default_graph

class Node(abc.ABC):
    &quot;&quot;&quot;Base Class&quot;&quot;&quot;

    def __init__(self, ins=None, name=None, updatable=False):
        &quot;&quot;&quot;
        init method
        :param ins: a list of Node instances (dependent nodes).
        :param name: the name of the node
        :param updatable: whether the node is updatable
        &quot;&quot;&quot;
        self._ins = ins if ins else []
        self._g = get_default_graph()
        self._name = name
        self._updatable = updatable
        self._reset()
        # Make sure everything is okay, then add node to the graph.
        self._g.add(self._ins, self)
        
    def _reset(self):
        &quot;&quot;&quot;Clear cache and reset upstream gradient.&quot;&quot;&quot;
        self._cache = None
        self._dout = np.zeros(self.shape, dtype=DEFAULTS[&#39;dtype&#39;])
        
    # ...
</code></pre>

<p><code>updatable</code>表明当前结点是否可更新，在优化的时候我们只需修改可更新的结点，<code>_reset</code>即清空缓存（见下文）并设置上游梯度为0，梯度的<code>shape</code>和本身的<code>shape</code>一致。</p>

<p>这里补充一句，我们具体的计算全部委托给了<code>numpy</code>，在<code>numpy</code>中，n维数组的<code>shape</code>是一个长度为n的元组，标量的<code>shape</code>是<code>()</code>空元组，因此我们只要统一全部用<code>numpy</code>的数据类型，这里的最后一行代码就可以兼容标量运算。</p>

<p>结点需要提供两个方法来支持前向后向传播：</p>

<pre><code>class Node(abc.ABC):    
    def eval(self):
        &quot;&quot;&quot;Evaluate the current node (cached).&quot;&quot;&quot;
        if self._cache is None:
            self._cache = self._eval()
        return self._cache
    
    @abc.abstractmethod
    def _eval(self):
        &quot;&quot;&quot;Evaluate the current node.&quot;&quot;&quot;

    def send(self, dout):
        &quot;&quot;&quot;Receive upstream gradients.&quot;&quot;&quot;
        self._dout += dout

    @abc.abstractmethod
    def propagate(self):
        &quot;&quot;&quot;Evaluate and propagate the gradient.&quot;&quot;&quot;
    
    # ...
</code></pre>

<p>结合上面<code>Graph</code>的<code>forward</code><code>backwark</code>方法代码一起看，在前向传播时就调用<code>eval</code>方法，为了防止重复计算，我们添加了缓存；后向传播时，调用<code>propagate</code>传播梯度；<code>send</code>方法是用来接收梯度的，原因上次讲过了：涉及到参数共享时，我们希望将梯度汇总完再进行传播。</p>

<p>这里有个很尴尬的命名问题，<code>send</code>方法实际上是接收梯度的，但名字却是send，这里主要参考了协程中的<code>send</code>命名。</p>

<p>这样，在子类中，我们只需实现<code>_eval</code>和<code>propagate</code>方法就好了。</p>

<h3 id="toc_2">Input与Variable</h3>

<p>计算图中一开始的数据哪里来？靠用户传入和变量，这里我们实现了2个子类：<code>Input</code>用于接收数据，例如传入mini batch；<code>Variable</code>用于模型参数和常量（通过<code>updatable</code>区分）。</p>

<pre><code>class Input(Node):
    &quot;&quot;&quot;Input Node&quot;&quot;&quot;

    def __init__(self, shape, name=None):
        &quot;&quot;&quot;
        init method
        :param shape: a tuple representing the shape of the input
        :param name:  the name of the node
        &quot;&quot;&quot;
        self._shape = shape
        super().__init__(name=name, updatable=False)

    def _eval(self):
        if self.graph.feed_dict is None:
            raise ValueError(&#39;Please provide the value of &quot;{!r}&quot;&#39;.format(self))
        return np.array(self.graph.feed_dict[self], dtype=DEFAULTS[&#39;dtype&#39;])

    def propagate(self):
        pass


class Variable(Node):
    &quot;&quot;&quot;Variable Node&quot;&quot;&quot;

    def __init__(self, init_val, name=None, updatable=True):
        &quot;&quot;&quot;
        init method
        :param init_val: initial value
        :param name: the name of the node
        :param updatable: whether the node represents a constant
        &quot;&quot;&quot;
        self._value = np.array(init_val, dtype=DEFAULTS[&#39;dtype&#39;])
        self._shape = self._value.shape
        super().__init__(name=name, updatable=updatable)

    @property
    def value(self):
        return self._value

    def _eval(self):
        return self._value

    def propagate(self):
        pass
</code></pre>

<p>实例化<code>Input</code>时，只需指定<code>shape</code>，且不依赖于任何结点，不可更新。前向传播时，从所属的<code>Graph</code>中的<code>feed_dict</code>寻找数据，<code>np.array</code>一来复制一份数据，防止奇怪的bug，二来将标量转换成<code>shape=()</code>的数组，方便后面的处理。</p>

<p>实例化<code>Variable</code>则需要提供初始值，<code>shape</code>则直接从初始值中获取。这两种结点都不依赖于其他结点，因此它们在反向传播时，什么都不需要做。</p>

<h3 id="toc_3">Op运算符</h3>

<p><code>Op</code>运算符，既然是运算，肯定要有输入，运算结果不可直接更新，而且运算结果的<code>shape</code>往往也是可以提前推断的：</p>

<pre><code>class Op(Node):
    &quot;&quot;&quot;Operator&quot;&quot;&quot;

    def __init__(self, ins, name=None):
        ins = [n if isinstance(n, Node) else Variable(n, updatable=False) for n in ins]
        self._shape = self._infer_shape(*ins)
        super().__init__(ins=ins, name=name, updatable=False)

    @abc.abstractmethod
    def _infer_shape(self, *ins):
        &quot;&quot;&quot;Infer node shape from inputs.&quot;&quot;&quot;
</code></pre>

<p>因此，我们的运算符还需要额外实现一个<code>_infer_shape</code>方法。在<code>__init__</code>中，我们将所有的不是<code>Node</code>的输入先转换成了不可更新的<code>Variable</code>充当常量，这样我们可以直接将<code>Node</code>和<code>np.ndarray</code>进行运算。</p>

<p>还有一点，我们的<code>super</code>都是写在最后的，这是因为基类<code>Node</code>的<code>__init__</code>会将结点添加进<code>Graph</code>中，我们希望一切安排妥当后再添加，防止添加进一个没用的结点。</p>

<h4 id="toc_4">矩阵乘法Matmul</h4>

<pre><code>class Matmul(Op):
    &quot;&quot;&quot;Matrix Multiplication&quot;&quot;&quot;

    def __init__(self, a, b, name=None):
        &quot;&quot;&quot;
        init method
        :param a: a Node or np.ndarray with shape=(m, n)
        :param b: a Node or np.ndarray with shape=(n, p)
        :param name: the name of the node 
        &quot;&quot;&quot;
        super().__init__(ins=[a, b], name=name)

    def _infer_shape(self, a, b):
        shape_a = a.shape
        shape_b = b.shape
        if len(shape_a) != 2 or len(shape_b) != 2:
            raise ValueError(&#39;Inputs of &quot;Matmul&quot; should be matrices.&#39;)
        if shape_a[1] != shape_b[0]:
            raise ValueError(&#39;shapes of &quot;{}&quot; and &quot;{}&quot; not aligned: {} x {}&#39;.format(
                a, b, shape_a, shape_b))
        return (shape_a[0], shape_b[1])

    def _eval(self):
        a, b = self._ins
        return a.eval() @ b.eval()

    def propagate(self):
        a, b = self._ins
        a.send(self._dout @ b.eval().T)
        b.send(a.eval().T @ self._dout)
</code></pre>

<p>我们这里实现了一个简单的矩阵乘法，强制要求输入为矩阵，即<code>len(shape)==2</code>。后向传播所依据的公式为<br/>
\[C=AB \rightarrow \nabla_AL=\nabla_CL \cdot B^T, \quad \nabla_BL=A^T \cdot \nabla_CL.\]</p>

<p><code>propagate</code>中我们将梯度传给了所依赖的两个结点，<code>send</code>方法永远是在上游结点中调用的，上游结点送出梯度，这样看来起名为<code>send</code>还是有点道理的XD。</p>

<h4 id="toc_5">需要Broadcasting的运算</h4>

<p>接下来我们需要实现一些elementwise的运算如加减乘除，看似简单，但因为涉及到了broadcasting，还是需要想一想的。</p>

<p>所谓broadcasting，即两个不同形状的数组进行elementwise运算，在满足一定条件下，可以将“较小”的数组复制多份并拼接（即broadcast），使其形状一致再进行运算。这里我画张图给大家感受下：</p>

<p><img src="http://o6jlkx4pl.bkt.clouddn.com/15004655001108.jpg" alt=""/></p>

<p>按照<code>numpy</code>的<a href="https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">broadcasting rules</a>，两个数组的<code>shape</code>（即两个元组），从后向前逐位比较，如果1.相等或2.有一个为1，则算为兼容，可继续向前比较，直到有一个元组耗尽。</p>

<p>如果两个数组是兼容的，则可以进行broadcast：1.将低维度数组在高维度上进行broadcast；2.在比较时遇到1的维度上进行broadcast。例如：</p>

<pre><code>shape(2, 3, 5) + shape(5,) = shape(2, 3, 5)
shape(2, 3, 5) + shape(3, 5) = shape(2, 3, 5)
shape(2, 3, 5) + shape(2, 1, 5) = shape(2, 3, 5)
shape(2, 3, 5) + shape(3) -&gt; incompatible
shape(1, 5) + shape(5, 1) = shape(5, 5)
</code></pre>

<p>因为规则2的关系，会出现这种<code>shape(1, 5) + shape(5, 1) = shape(5, 5)</code>，处理起来比较麻烦，我们就暂不考虑了。</p>

<p>现在我们弄明白broadcasting的规则，就要想一下broadcast之后的运算，梯度怎么求。其实也很简单，因为broadcast实际上就是将原始数据复制多份，类似参数共享，那我们只需要将梯度沿之前broadcast过的维度加起来就好了。从数学的角度看这个问题，只考虑2维数组（矩阵）和1维数组（向量）的运算</p>

<p>\[<br/>
A \circ b^T = <br/>
    \begin{bmatrix}<br/>
        a_1^T \\<br/>
        a_2^T \\<br/>
        \cdots \\<br/>
        a_m^T<br/>
    \end{bmatrix} \circ b^T =<br/>
\begin{bmatrix}<br/>
        a_1^T \circ b^T\\<br/>
        a_2^T \circ b^T\\<br/>
        \cdots \\<br/>
        a_m^T \circ b^T<br/>
    \end{bmatrix} = A \circ (e b^T) = A\circ D,<br/>
\]<br/>
其中\(D = e b^T\)，\(e\)为全为1的列向量，\(\circ\)为某elementwise运算。这样，在我们得到关于\(D\)的梯度\(\nabla_D\)后，很容易算出关于\(b^T\)的梯度\(\nabla_{b^T} = e^T\nabla_D\)，即对矩阵\(\nabla_D\)进行逐列求和。</p>

<p><img src="http://o6jlkx4pl.bkt.clouddn.com/15004655223020.jpg" alt=""/></p>

<p>代码如下：</p>

<pre><code>class BroadcastMixin:
    &quot;&quot;&quot;a mixin class providing &#39;_infer_shape&#39; method&quot;&quot;&quot;

    def _infer_shape(self, a, b):
        &quot;&quot;&quot;Infer shape according to numpy broadcasting rule.&quot;&quot;&quot;
        shape_a = a.shape
        shape_b = b.shape
        rank_a = len(shape_a)
        rank_b = len(shape_b)
        self._sum_over_a = []  # axes along which a sum is performed
        self._sum_over_b = []
        if rank_a &gt; rank_b:
            ret_shape = list(shape_a)
            self._squeeze_over_a = ()  # axes that will be removed
            self._squeeze_over_b = tuple(range(rank_a - rank_b))
        else:
            ret_shape = list(shape_b)
            self._squeeze_over_a = tuple(range(rank_b - rank_a))
            self._squeeze_over_b = ()
        for d_a, d_b, k in zip(reversed(shape_a),
                               reversed(shape_b),
                               range(max(rank_a, rank_b)-1, -1, -1)):
            if d_a == d_b:  # != 1
                continue
            elif d_a == 1:
                ret_shape[k] = d_b
                self._sum_over_a.append(k)
            elif d_b == 1:
                ret_shape[k] = d_a
                self._sum_over_b.append(k)
            else:
                raise ValueError(&#39;operands could not be &#39;
                                 &#39;broadcast together with &#39;
                                 &#39;shapes {} {}&#39;.format(shape_a, shape_b))
        self._sum_over_a = self._squeeze_over_a + tuple(reversed(self._sum_over_a))
        self._sum_over_b = self._squeeze_over_b + tuple(reversed(self._sum_over_b))
        return tuple(ret_shape)

    def _sum_gradient_and_send(self, da, db):
        &quot;&quot;&quot;Sum the gradients and then send them.&quot;&quot;&quot;
        a, b = self._ins
        if self._sum_over_a:
            summed_da = da.sum(axis=self._sum_over_a, keepdims=True)
            summed_da = np.squeeze(summed_da, axis=self._squeeze_over_a)
        else:
            summed_da = da
        if self._sum_over_b:
            summed_db = db.sum(axis=self._sum_over_b, keepdims=True)
            summed_db = np.squeeze(summed_db, axis=self._squeeze_over_b)
        else:
            summed_db = db
        a.send(summed_da)
        b.send(summed_db)
</code></pre>

<p>这里我搞了一个混入类，不知道合不合适，主要是不想再继承<code>Op</code>了。这个类提供2个方法：<code>_infer_shape</code>推断形状，<code>_sum_gradient_and_send</code>对梯度求和并<code>send</code>给之前的结点。</p>

<p><code>_infer_shape</code>推断形状主要就是<code>if</code>那一块，但大部分代码都用来记录两组变量，<code>_sum_over_*</code>与<code>_squeeze_over_*</code>。接收到完整梯度（上文的\(\nabla_D\)）后，<code>_sum_over_*</code>告诉我们要对哪几个维求和，由于我们调用<code>sum</code>时令<code>keepdims=True</code>，这意味着求完和后，还要消除掉冗余的维，冗余的边由<code>_squeeze_over_*</code>提供。</p>

<p>举个例子：</p>

<blockquote>
<p>a.shape = (5, 1, 3)<br/>
b.shape = (2, 3)<br/>
c = a+b; c.shape = (5, 2, 3)<br/>
a的第1维会被broadcast，因此<code>sum_over_a=(1,)</code>，<code>squeeze_over_a=()</code><br/>
因为a的维数比b的高，b会新增一个第0维并broadcast，所以<code>squeeze_over_a=()</code>，<code>squeeze_over_b=(0,)</code><br/>
...<br/>
接收到梯度：da.shape=(5, 2, 3), db.shape=(5, 2, 3)<br/>
<code>summed_da = da.sum(axis=(1,), keepdims=True)</code><br/>
summed_da.shape=(5, 1, 3) 形状与a一致，收工<br/>
<code>summed_db = db.sum(axis=(0,), keepdims=True)</code><br/>
summed_db.shape=(1, 2, 3) 多了一维<br/>
<code>summed_db = np.squeeze(summed_db, axis=(0,))</code><br/>
summed_db.shape=(2, 3) 去掉第0维，形状与b一致，收工！</p>
</blockquote>

<p>这样，基础的四则运算代码就很简单啦，这里只放上加法的代码：</p>

<pre><code>class Add(BroadcastMixin, Op):

    def __init__(self, a, b, name=None):
        super().__init__(ins=[a, b], name=name)

    def _eval(self):
        a, b = self._ins
        return a.eval() + b.eval()

    def propagate(self):
        a, b = self._ins
        self._sum_gradient_and_send(self._dout, self._dout)
</code></pre>

<h4 id="toc_6">Reduce</h4>

<p><code>Reduce</code>类运算是一类非常关键的运算，如求和求均值取最大值等，不过实现也简单，就不多说了，这里放上求均值的代码。</p>

<pre><code>class ReduceMean(Op):

    def __init__(self, a, name=None):
        super().__init__(ins=[a], name=name)

    def _infer_shape(self, a):
        return ()

    def _eval(self):
        a = self._ins[0]
        return a.eval().mean()

    def propagate(self):
        a = self._ins[0]
        n = np.product(a.shape)
        a.send(np.full(a.shape, fill_value=self._dout / n, dtype=DEFAULTS[&#39;dtype&#39;]))
</code></pre>

<h4 id="toc_7">其他运算符</h4>

<p>幂的代码就写在这里了，没太多好说的，唯一需要注意的就是，我偷懒，没把p设置为一个Node，不过对于线性回归来说已经够了。类似的，指数运算也很好写，下次再写好了XD</p>

<pre><code>class Pow(Op):

    def __init__(self, a, p, name=None):
        &quot;&quot;&quot;
        Calculate a ** p.
        :param a: a Node instance
        :param p: a real number
        :param name: the name of Node
        &quot;&quot;&quot;
        super().__init__(ins=[a], name=name)
        self._p = p

    def _infer_shape(self, a):
        return a.shape

    def _eval(self):
        a = self._ins[0]
        return a.eval() ** self._p

    def propagate(self):
        a = self._ins[0]
        a.send(self._p * self._dout * a.eval() ** (self._p - 1))
</code></pre>

<h3 id="toc_8">重载运算符</h3>

<p>为了方便代码书写，我们回过头将<code>Node</code>类型的运算符重载了：</p>

<pre><code>class Node:
    def __matmul__(self, b):
        return Matmul(self, b)

    def __add__(self, b):
        return Add(self, b)

    def __sub__(self, b):
        return Subtract(self, b)

    def __rsub__(self, a):
        return Subtract(a, self)

    def __mul__(self, b):
        return Multiply(self, b)

    def __truediv__(self, b):
        return Divide(self, b)

    def __rtruediv__(self, a):
        return Divide(a, self)

    def __pow__(self, power, modulo=None):
        return Pow(self, power)
</code></pre>

<h2 id="toc_9">实战：线性回归</h2>

<p>到了实战环节，我们要用之前构建的代码，实现最简单的线性回归。<a href="https://github.com/YaoZh1918/PyNeuralNet/blob/master/pynn_v1_demo.ipynb">notebook</a>在这里。</p>

<h3 id="toc_10">生成数据</h3>

<pre><code>from matplotlib import pyplot as plt
import numpy as np
import neuralnet_v1 as pynn

# generate data
def batch_generator(batch_size, weight, bias):
    n = len(weight)
    while True:
        X = np.random.rand(batch_size, n)
        y = X @ weight + bias + np.random.rand(batch_size) * .01
        yield X, y

batch_size = 128
n = 50

w_true = np.random.randn(n, 1)
b_true = np.random.randn()
batch_iter = batch_generator(batch_size, w_true, b_true)
</code></pre>

<h3 id="toc_11">定义计算图</h3>

<p>类似tensorflow，<code>with graph.as_default():</code>之后，把定义写在代码块中就好。</p>

<pre><code># Define Graph

graph = pynn.Graph()
with graph.as_default():
# with pynn.Graph().as_default() as graph:  # alternative
    X_input = pynn.Input(shape=(batch_size, n), name=&#39;Input_X&#39;)
    y_input = pynn.Input(shape=(batch_size,), name=&#39;Input_y&#39;)
    W = pynn.Variable(np.random.randn(n, 1), name=&#39;Weights&#39;)
    b = pynn.Variable(np.random.randn(), name=&#39;Bias&#39;)
    y_pred = X_input @ W + b
    loss = pynn.ReduceMean(((y_pred - y_input) ** 2), name=&#39;loss&#39;)
</code></pre>

<p>定义好后可以可视化一下我们的计算图：</p>

<p><img src="http://o6jlkx4pl.bkt.clouddn.com/15004746401403.jpg" alt="Computation Graph"/></p>

<p>还不错，可惜<code>networkx</code>画图不够漂亮。</p>

<h3 id="toc_12">优化</h3>

<p>优化就用最简单的SGD，这里可以看到我们的<code>one_pass</code>的用法。</p>

<pre><code>alpha = .01
err = []
for i in range(5000):
    X_tr, y_tr = next(batch_iter)
    with graph.one_pass(feed_dict={X_input: X_tr, y_input: y_tr}):
        graph.forward()
        err.append(loss.eval())
        graph.backward()
        for node in graph.updatable_nodes:
            node._value -= alpha * node.gradient
</code></pre>

<p>最后画一下损失函数的图：</p>

<p><img src="http://o6jlkx4pl.bkt.clouddn.com/15004747430538.jpg" alt="Loss"/></p>

<p>还不错哟！</p>

<p>这期就到这里了，下次我们将计划实现卷积操作，并在MNIST数据集上进行实验。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python实现人工神经网络(1)]]></title>
    <link href="http://yaozh1918.github.io/15000325978554.html"/>
    <updated>2017-07-14T19:43:17+08:00</updated>
    <id>http://yaozh1918.github.io/15000325978554.html</id>
    <content type="html"><![CDATA[
<p>最近刚听完<a href="https://cs231n.github.io">cs231n</a>，突然想自己实现一下神经网络，故开这么一个坑。repo在<a href="https://github.com/YaoZh1918/PyNeuralNet">这里</a>。基本想法就是仿照Tensorflow和Theano实现一个计算图(Computation Graph)，可以自动传梯度。自己写有什么好处呢，其一是开心XD，另外就是可以加深理解，比方说为什么Deconv叫反卷积，为什么有一群人在争论Deconv是不是个好名字，自己写一下Conv和Deconv的forward和backward操作就明白了。</p>

<p>这个系列的第一篇文章就实现下最简单的计算图，都是标量运算，主要理解下BackPropagation的思想。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_0">BP与Chain Rule</h2>

<p>这里简单讲一下BP的思想，毕竟已经有很多专门介绍的文章了。</p>

<p>考虑一个简单的模型(来源cs231n)：神经网络中的某一个神经元，接受两个输入\(x,y\)，在其内部经过处理，变成\(z=f(x,y)\)，其输出\(z\)再经过若干神经元，变成了网络的最终输出（通常是损失函数的值）\(f_1\circ f_2 \circ\cdots \circ f_k(z, ...)=g(z, ...)=L\)，假设我们已经得到了损失关于\(z\)的偏导\(\frac{\partial L}{\partial z}\)（可以称为上游梯度，upstream gradient，因为也是其他上游神经元传来的），那计算\(\frac{\partial L}{\partial x}\)与\(\frac{\partial L}{\partial y}\)可直接通过链式法则求得：</p>

<p>\[\frac{\partial L}{\partial x}=\frac{\partial L}{\partial z} \frac{\partial z}{\partial x}, \quad <br/>
\frac{\partial L}{\partial y}=\frac{\partial L}{\partial z} \frac{\partial z}{\partial y},\]</p>

<p>其中，\(\frac{\partial z}{\partial x}\)与\(\frac{\partial z}{\partial y}\)可以看作是local gradients，在进行前向传播时就可以算好，缓存在神经元中，这也是为什么会说反向传播需要前向时2倍存储与计算量。</p>

<p>现在我们就得到了这样一个递归的形式，算出来的\(\frac{\partial L}{\partial x}\)与\(\frac{\partial L}{\partial y}\)可以继续反向传给之前的神经元，起始状态也很简单：\(\frac{\partial L}{\partial L}=1\)，这样很方便就可以算出整个网络参数的梯度。</p>

<p><img src="http://o6jlkx4pl.bkt.clouddn.com/15000121481274.jpg" alt=""/></p>

<h2 id="toc_1">标量计算图</h2>

<p>（代码在<a href="https://github.com/YaoZh1918/PyNeuralNet/blob/master/scalar_graph_v1.ipynb">notebook</a>中）</p>

<p>例子就用cs231n中的：</p>

<p>\[f(w,x) = \frac{1}{1 + \exp[-(w_1x_1 + w_2x_2 + b)]}.\]</p>

<p>针对这个问题，我们只需要实现4个运算：加、乘、取逆、指数。也就是说，我们的计算图中只需要这4种运算结点，外加一类输入结点。这些结点最少也要有两个方法：<code>forward()</code>与<code>backward()</code>：</p>

<pre><code>import numpy as np
import abc


class Node(abc.ABC):
    
    @abc.abstractmethod
    def forward(self):
        &quot;&quot;&quot;Feed Forward&quot;&quot;&quot;
    
    @abc.abstractmethod
    def backward(self, dout):
        &quot;&quot;&quot;Back Propagate
        Inputs:
            dout: upstream gradient
        &quot;&quot;&quot;

    @property
    def grads(self):
        return self._grads
</code></pre>

<p>最简单的，Variable类型的结点，可以充当输入、常量（不可更新）或参数（可更新），<code>forward()</code>只需返回当前值，<code>backward()</code>也只是将上流梯度作为自己的梯度即可，不需要继续后向传播，即后向传播递归过程的终止状态。</p>

<pre><code>class Variable(Node):
    
    def __init__(self, val):
        self._v = v
    
    def forward(self):
        return self._v
    
    def backward(self, dout):
        self._grads = dout
</code></pre>

<p>接下来两个二元运算，都接受两个<code>Node</code>，在<code>forward()</code>时，递归调用这两个的<code>forward()</code>方法，获取他们的值，再进行相应计算并返回；<code>backward()</code>时，利用local和upstream梯度算出当前梯度，并向后传播。这里我采用的方法是前向时计算local gradient而不缓存原始数据。</p>

<pre><code>class Add(Node):
    
    def __init__(self, a, b):
        self._a = a
        self._b = b
    
    def forward(self):
        v_a = self._a.forward()
        v_b = self._b.forward()
        # self._local_da = 1.
        # self._local_db = 1.
        return v_a + v_b
    
    def backward(self, dout):
        self._grads = [dout, dout]
        self._a.backward(dout)
        self._b.backward(dout)


class Mul(Node):
    
    def __init__(self, a, b):
        self._a = a
        self._b = b
    
    def forward(self):
        v_a = self._a.forward()
        v_b = self._b.forward()
        self._local_da = v_b
        self._local_db = v_a
        return v_a * v_b
    
    def backward(self, dout):
        da = self._local_da * dout
        db = self._local_db * dout
        self._grads = [da, db]
        self._a.backward(da)
        self._b.backward(db)
</code></pre>

<p>最后两个一元运算也类似：</p>

<pre><code>class Inv(Node):
    
    def __init__(self, a):
        self._a = a
    
    def forward(self):
        val = self._a.forward()
        self._local_grads = - 1. / val**2
        return 1. / val
    
    def backward(self, dout):
        self._grads = self._local_grads * dout
        self._a.backward(self._grads)
        

class Exp(Node):
    
    def __init__(self, a):
        self._a = a
    
    def forward(self):
        val = self._a.forward()
        self._local_grads = np.exp(val)
        return self._local_grads
    
    def backward(self, dout):
        self._grads = self._local_grads * dout
        self._a.backward(self._grads)
</code></pre>

<p>最后我们做下测试：</p>

<pre><code class="language-python3"># Init variables
w1, w2, x1, x2, b = [Variable(float(i)) for i in [2, -3, -1, -2, -3]]

# build graph
logit = Add(Add(Mul(w1, x1), Mul(w2, x2)), b)
f = Inv(Add(Variable(1), Exp(Mul(logit, Variable(-1)))))

# Eval
print(&#39;Values: \n&#39;, logit.forward(), f.forward())
# BP
f.backward(1.0)
print(&#39;Gradients: &#39;)
print(&#39;, &#39;.join(&#39;{:.2f}&#39;.format(v.grads) for v in [w1, w2, x1, x2, b]))
</code></pre>

<blockquote>
<p>Values: <br/>
 1.0 0.73105857863<br/>
Gradients: <br/>
-0.20, -0.39, 0.39, -0.59, 0.20</p>
</blockquote>

<p>对比下正确答案，完美！</p>

<p><img src="http://o6jlkx4pl.bkt.clouddn.com/15000149840268.jpg" alt=""/></p>

<h2 id="toc_2">参数共享</h2>

<p>上面通过递归的方式调用，稍微会有点小问题，一是递归过深python会报错（虽然我们这里不太可能发生），另一点就是参数共享时会出错，考虑这样的一个问题：<br/>
\[f = wx_1 + wx_2\]</p>

<pre><code>w, x1, x2 = [Variable(float(i)) for i in [5, 1, 2]]
f = Add(Mul(w, x1), Mul(w, x2))
print(&#39;Value: \n&#39;, f.forward())
f.backward(1.)
print(&#39;Gradients: \n&#39;, [v.grads for v in [w, x1, x2]])
</code></pre>

<p>输出：</p>

<blockquote>
<p>Value: <br/>
 15.0<br/>
Gradients: <br/>
 [2.0, 5.0, 5.0]</p>
</blockquote>

<p>很明显，关于w的梯度算错了，这是因为w接受自上游的梯度应该相加。具体来说，给定一个计算图，它肯定是有向无环图，在前向传播时，按照拓扑排序来算，后向时则反着来，在未遍历到某个节点前，该结点都只累计上游梯度，不计算，遍历到了再计算，并传播。</p>

<p>上面的代码我们可以稍作修改，主要两点改动：<br/>
1. 维护一个列表，记录结点的创建顺序，即拓扑排序；<br/>
2. 新增propagate方法累积梯度，不传播</p>

<p>这里只改了两个二元运算，完整的代码在<a href="https://github.com/YaoZh1918/PyNeuralNet/blob/master/scalar_graph_v2.ipynb">github</a>中。</p>

<pre><code>import numpy as np
import abc


topo_list = []  # Topological sort

class Node(abc.ABC):
    
    def __init__(self):
        topo_list.append(self)
        self._dout = 0.0
    
    @abc.abstractmethod
    def forward(self):
        &quot;&quot;&quot;Feed Forward&quot;&quot;&quot;
    
    @abc.abstractmethod
    def backward(self):
        &quot;&quot;&quot;Back Propagate&quot;&quot;&quot;
    
    def as_terminal(self):
        &quot;&quot;&quot;Let the node be the terminal &quot;&quot;&quot;
        self._dout = 1.
        return self
    
    def propagte(self, dout):
        &quot;&quot;&quot;Aggregate upstream gradients.&quot;&quot;&quot;
        self._dout += dout

    @property
    def grads(self):
        return self._grads


class Variable(Node):
    
    def __init__(self, val):
        super().__init__()
        self._v = val
    
    def forward(self):
        return self._v
    
    def backward(self):
        self._grads = self._dout

        
class Add(Node):
    
    def __init__(self, a, b):
        super().__init__()
        self._a = a
        self._b = b
    
    def forward(self):
        v_a = self._a.forward()
        v_b = self._b.forward()
        return v_a + v_b
    
    def backward(self):
        dout = self._dout
        self._grads = [dout, dout]
        self._a.propagte(dout)
        self._b.propagte(dout)


class Mul(Node):
    
    def __init__(self, a, b):
        super().__init__()
        self._a = a
        self._b = b
    
    def forward(self):
        v_a = self._a.forward()
        v_b = self._b.forward()
        self._local_da = v_b
        self._local_db = v_a
        return v_a * v_b
    
    def backward(self):
        dout = self._dout
        da = self._local_da * dout
        db = self._local_db * dout
        self._grads = [da, db]
        self._a.propagte(da)
        self._b.propagte(db)
        
</code></pre>

<p>测试:</p>

<pre><code>topo_list = []

w, x1, x2 = [Variable(float(i)) for i in [5, 1, 2]]
f = Add(Mul(w, x1), Mul(w, x2))
print(&#39;Value: \n&#39;, f.forward())

f.as_terminal()
for v in reversed(topo_list):
    v.backward()
print(&#39;Gradients: \n&#39;, [v.grads for v in [w, x1, x2]])
</code></pre>

<blockquote>
<p>Value: <br/>
 15.0<br/>
Gradients: <br/>
 [3.0, 5.0, 5.0]</p>
</blockquote>

<p>完全正确！要注意，因为在实例化时我们把上流梯度设置为了0，在反向传播前，要通过<code>.as_terminal()</code>先将最后一个结点的上流梯度设置为1（即\(\frac{\partial L}{\partial L}=1\))。</p>

<p>不过现在代码还是有点丑陋，功能也很欠缺，那么在下一篇文章中，我们将实现基本的矩阵运算的结点，同时要支持batch，再引入一个类似Tensorflow的Graph来管理计算图。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[利用Ngrok实现内网穿透]]></title>
    <link href="http://yaozh1918.github.io/14836189296241.html"/>
    <updated>2017-01-05T20:22:09+08:00</updated>
    <id>http://yaozh1918.github.io/14836189296241.html</id>
    <content type="html"><![CDATA[
<p>放假快回家了，想着回家就用不了学校的机器了，所以打算内网穿透试一下，折腾一下午总算搞定了。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_0">前期准备</h2>

<p>首先你要有一台有公网ip的server，同时还要有个域名。server我是用腾讯云学生特惠1块钱搞定的，域名是之前在namecheap上买的，一刀左右一年。server端和client端我都用的是<code>Ubuntu LTS 16.04 64</code>的系统，这样之后编译Ngrok时会方便一些。</p>

<h2 id="toc_1">编译Ngrok</h2>

<p>这一节内容主要参考了这篇文章：<a href="http://www.tuicool.com/articles/ZraURrq">在阿里云搭建自己的ngrok服务</a></p>

<p>因为Ngrok是用go开发的，所以我们需要安装一下go的环境，看github上的说明，还需要装一个<code>mercurial</code>也不知道是什么，都装了肯定不会错。</p>

<pre><code>sudo apt install golang mercurial
</code></pre>

<p>对了，如果是低版本的centos系统，还需要自己升级一下<code>git</code>，<code>yum</code>里的版本比较低，否则后面编译的时候会出错。</p>

<p>之后就是clone下repo，准备编译：</p>

<pre><code>export NGROK_DOMAIN=&quot;ngork.mydomain.com&quot;
git clone https://github.com/inconshreveable/ngrok.git
cd ngrok
</code></pre>

<p>注意把<code>NGROK_DOMAIN</code>替换成自己的域名。</p>

<p>然后生成自签名证书并拷贝到正确的位置：</p>

<pre><code>openssl genrsa -out rootCA.key 2048

openssl req -x509 -new -nodes -key rootCA.key -subj &quot;/CN=$NGROK_DOMAIN&quot; -days 5000 -out rootCA.pem

openssl genrsa -out device.key 2048

openssl req -new -key device.key -subj &quot;/CN=$NGROK_DOMAIN&quot; -out device.csr

openssl x509 -req -in device.csr -CA rootCA.pem -CAkey rootCA.key -CAcreateserial -out device.crt -days 5000

cp rootCA.pem assets/client/tls/ngrokroot.crt

cp device.crt assets/server/tls/snakeoil.crt

cp device.key assets/server/tls/snakeoil.key 
</code></pre>

<p>之后就可以编译server端和client端：</p>

<pre><code>make release-server release-client
</code></pre>

<p>如果server和client系统不一样，需要在前面指定。如生成64位macos客户端：</p>

<pre><code>GOOS=darwin GOARCH=amd64 make release-client
</code></pre>

<p><code>GOOS</code>和<code>GOARCH</code>的取值可通过<code>go env</code>查看。</p>

<p>如果一切顺利，会生成两个文件：<code>ngrokd</code>留在server上，<code>ngrok</code>拷贝到client上。</p>

<h2 id="toc_2">开启server服务</h2>

<p>在server上，通过以下命令开启服务：</p>

<pre><code>export NGROK_DOMAIN=&quot;ngork.mydomain.com&quot;
./ngrokd -domain=&quot;$NGROK_DOMAIN&quot; -httpAddr=&quot;:8001&quot; -httpsAddr=&quot;:8002&quot; -tunnelAddr=&quot;:8443&quot; -log=ngrokd.log -log-level=&quot;INFO&quot; &amp;
</code></pre>

<p><code>domain</code>肯定是要指定的，接下来三个端口可以用默认值，不过我习惯改一下，另外我们希望log输出到文件中，而不是stdout上，最后末尾的<code>&amp;</code>是为了让服务在后台运行。</p>

<h2 id="toc_3">开启client服务</h2>

<p>在client上，我们通过<code>ngrok</code>来实现内网穿透，首先新建<code>~/.ngrok</code>文件，写入以下内容：</p>

<pre><code>server_addr: &quot;ngrok.mydomain.com:8443&quot;
trust_host_root_certs: false
</code></pre>

<p>注意addr后面的端口要与server上的tunnelAddr相同。</p>

<p>当然配置文件也可以另建一个，运行ngrok的时候用<code>-config=</code>传入就好。</p>

<p>现在我们已经可以用了：</p>

<pre><code>./ngrok -subdomain=test 80
</code></pre>

<p>这时ternimal会显示一些内容，当Tunnel Status变为online就连接成功了，这样我们访问<code>http://test.ngrok.mydomain.com:8001</code>就会被指向<code>127.0.0.1:80</code>。程序默认还会创建一个web interface，可以通过<code>127.0.0.1:4040</code>来访问，但没什么用。</p>

<h2 id="toc_4">配置config文件</h2>

<p>如果我们想同时把多个端口映射到server上怎么办？当然可以打开多个<code>ngrok</code>，不过更方便的做法是把它们都写在配置文件中。</p>

<p><a href="https://ngrok.com/docs#multiple-tunnels">官网的材料</a>是关于2.0版本的，而2.0版本没有开源，github上的是1.7版本的，所以写法还不一样，之前一直报错，读了<a href="https://github.com/inconshreveable/ngrok/blob/master/src/ngrok/client/config.go#L124">源码</a>才知道这个地方该怎么写。ps. 配置文件其实是yaml格式的。</p>

<p>比方说我们想创建3个tunnels，一个同时映射80和443端口，一个映射web interface，一个映射22用来ssh连接，我们可以这样写：</p>

<pre><code>server_addr: &quot;ngrok.mydomain.com:8443&quot;
trust_host_root_certs: false
tunnels:
    web-app:
        subdomain: web-app
        proto:
            http: 80
            https: 443
    web-interface:
        subdomain: web-interface
        proto:
            http: 4040
    ssh:
        proto:
            tcp: 22
        remote_port: 8022
</code></pre>

<p>这样如果我想运行其中两个，可以写</p>

<pre><code>./ngrok start web-app ssh
</code></pre>

<p>如果想全部运行，可以写</p>

<pre><code>./ngrok start-all
</code></pre>

<p>这样，我们访问<code>http://wep-app.ngrok.mydomain.com</code>和<code>https://wep-app.ngrok.mydomain.com</code>就可以访问内网架的网页，访问<code>http://web-interface.ngrok.mydomain.com</code>可以访问web interface。注意ssh不需要声明<code>subdomain</code>，但最好给定<code>remote_port</code>，这样ssh就可以这样执行<code>ssh user@ngrok.mydomain.com -p 8022</code>！</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Smooth Thresholding Function]]></title>
    <link href="http://yaozh1918.github.io/14791104542390.html"/>
    <updated>2016-11-14T16:00:54+08:00</updated>
    <id>http://yaozh1918.github.io/14791104542390.html</id>
    <content type="html"><![CDATA[
<p>最近读paper<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup>的时候，看到了一个有趣的函数，可以看成thresholding function的一个光滑的近似，故我决定写篇文章简单介绍一下。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_0">原函数</h2>

<p>文中给出的这个函数是这样的：</p>

<p>\[f(x)=0.25\ln(\cosh(2x))+0.5x+0.17.\]</p>

<p>图像是这样的：</p>

<p><img src="media/14791104542390/origin.png" alt="origin"/></p>

<p>很神奇是不是！但我们还是要研究下这个函数为什么就work？</p>

<p>首先函数中的那个0.17是怎么来的？从图像上观察，应该是进行平移保证整个函数是非负的。</p>

<p>令\(g(x)=\ln \cosh (x) + x\)，这样\(f(x)=\frac{1}{4}g(2x) + 0.17\)。那我们先来看下\(g(x)\)趋于负无穷时的极限：</p>

<p>\[<br/>
\begin{align*}<br/>
 &amp; \lim_{x\to -\infty}\ln \cosh(x) + x \\<br/>
=&amp; \lim_{x\to -\infty} \ln \frac{e^{-x} + e^x}{2} + x \\<br/>
=&amp; \lim_{x\to -\infty} \ln \exp (\ln(e^{-x} + e^x) + x) - \ln 2\\<br/>
=&amp; \lim_{x\to -\infty} \ln(1+e^{2x}) - \ln 2\\<br/>
=&amp; -\ln 2<br/>
\end{align*}<br/>
\]</p>

<p>这样\(\frac{1}{4}g(2x) = -\frac{1}{4}\ln 2\approx -0.17\)，所以这个函数最精确的表达应该是</p>

<p>\[f(x)=0.25\ln(\cosh(2x))+0.5x+0.25\ln2.\]</p>

<h2 id="toc_1">导函数</h2>

<p>接下来我们换个角度，一般的thresholding function是</p>

<p>\[h(x)=\left\{<br/>
\begin{aligned}<br/>
0 &amp; \qquad \mathrm{if}~ x &lt; 0\\<br/>
x &amp; \qquad \mathrm{if}~ x \ge 0,<br/>
\end{aligned}<br/>
\right.<br/>
\]</p>

<p>其导数是<br/>
\[h&#39;(x)=\left\{<br/>
\begin{aligned}<br/>
0 &amp; \qquad \mathrm{if}~ x &lt; 0\\<br/>
1 &amp; \qquad \mathrm{if}~ x &gt; 0.<br/>
\end{aligned}<br/>
\right.<br/>
\]</p>

<p>而这个光滑版的thresholding function的导数是</p>

<p>\[f&#39;(x)=0.5\tanh(2x)+0.5,\]</p>

<p><img src="media/14791104542390/tanh.png" alt="tanh"/></p>

<p>可以看作是\(h&#39;(x)\)的光滑近似，图像上看就是sigmoid形的。它保证了\(x&lt;0\)时，随着\(x\)减小，\(f&#39;(x)\)能很快的下降到接近于0，反映到原函数上就是此时函数几乎无增长；在\(x&gt;0\)时，随着\(x\)增大，\(f&#39;(x)\)能很快的增长到1，这样原函数就有着线性的增长率。</p>

<h2 id="toc_2">设计更多的smooth thresholding functions</h2>

<p>上一节最后给我们提供了一个角度，我们先选择一个长得像sigmoid的函数，然后求积分，就可以得到一个smooth thresholding function。</p>

<p>例如可以取sigmoid函数\(f&#39;(x)=\frac{1}{1+e^x}\)，这样得到\(f(x)=\ln(1+e^x)\).</p>

<div class="footnotes">
<hr/>
<ol>

<li id="fn1">
<p>M.U. Gutmann and A. Hyvärinen. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. Journal of Machine Learning Research, 13:307–361, 2012.&nbsp;<a href="#fnref1" rev="footnote">&#8617;</a></p>
</li>

</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ubuntu上安装Transmission Web界面]]></title>
    <link href="http://yaozh1918.github.io/14785830863216.html"/>
    <updated>2016-11-08T13:31:26+08:00</updated>
    <id>http://yaozh1918.github.io/14785830863216.html</id>
    <content type="html"><![CDATA[
<p>考虑到实验室的电脑放着也是浪费，打算装个BT客户端下点种子，同时还可以为我的PT加点流量。但是，实验室的电脑一般就放置在那里，不想动它，最好有个可以远程操控的客户端，经过一番搜寻，打算用Transmission加web界面来实现。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_0">Install</h2>

<p>首先安装Transmission，Ubuntu的repo里已经有Transmission了，如果想要最新版本也可以添加PPA：</p>

<pre><code>sudo add-apt-repository ppa:transmissionbt/ppa
sudo apt-get update
</code></pre>

<p>之后安装：</p>

<pre><code>sudo apt-get install transmission-cli transmission-common transmission-daemon
</code></pre>

<h2 id="toc_1">Configure</h2>

<p>安装好以后其实已经可以用了，在本地上访问<code>localhost:9091</code>，默认用户名密码都是<code>transmission</code>，但因为白名单的缘故，远程无法访问。为此，我们需要修改位于<code>/var/lib/transmission-daemon/info/settings.json</code>的配置文件。在此之前，记得先停止daemon：</p>

<pre><code>sudo service transmission-daemon stop
</code></pre>

<p>那么在配置文件中，有哪些需要修改呢？<br/>
首先添加白名单并修改账号密码，例如：</p>

<pre><code>&quot;rpc-password&quot;: &quot;password&quot;,
&quot;rpc-username&quot;: &quot;user&quot;,
&quot;rpc-whitelist&quot;: &quot;127.0.0.1,192.168.*.*&quot;,
</code></pre>

<p>注意，默认的密码是SHA1加密后的，我们在修改的时候无需手动加密，直接改就好了，之后重启daemon会自动进行加密。</p>

<p>然后还要修改<code>umask</code>参数，以保证Transmission创建的文件其他用户也可以访问：</p>

<pre><code>&quot;umask&quot;: 2,
</code></pre>

<p>都修改好之后，重启daemon：</p>

<pre><code>sudo service transmission-daemon start
</code></pre>

<p>现在，远程用户（白名单中）可以访问<code>http://server-ip:9091</code>来控制Transmission了。</p>

<h2 id="toc_2">自定义下载位置</h2>

<p>默认的下载位置在<code>/var/lib/transmission-daemon/downloads/</code>，有那么一丝丝不方便，可能需要修改到我们当前用户的<code>Downloads</code>目录下，这一小节就介绍下如何修改下载位置。</p>

<p>首先，出于安全因素，Transmission有自己的一个用户，而现在我们需要让Transmission将文件下载至当前用户的目录下，那么首先需要将当前用户加至组里（假设当前用户叫user）：</p>

<pre><code>sudo usermod -a -G debian-transmission user
</code></pre>

<p>然后我们在当前用户的下载目录中新建一些文件夹，并修改权限：</p>

<pre><code>cd /home/user/Downloads
mkdir transmission
cd transmission
mkdir completed incomplete torrents

sudo chgrp -R debian-transmission /home/user/Downloads/transmission
sudo chmod -R 775 /home/user/Downloads/transmission
</code></pre>

<p>最后修改<code>settings.json</code>:</p>

<pre><code>&quot;download-dir&quot;: &quot;/home/user/Downloads/transmission/completed&quot;,
&quot;incomplete-dir&quot;: &quot;/home/user/Downloads/transmission/incomplete&quot;,
&quot;incomplete-dir-enabled&quot;: true,
&quot;watch-dir&quot;: &quot;/home/user/Downloads/transmission/torrents&quot;,
&quot;watch-dir-enabled&quot;: true
</code></pre>

<p>这样，下载中的文件都会在incomplete中，下载好的在completed中，将种子移至torrents文件夹就会自动开始下载。</p>

<p>收工！</p>

<h2 id="toc_3">Reference</h2>

<p>本文主要参考了以下两篇文章：<br/>
<a href="https://help.ubuntu.com/community/TransmissionHowTo">TransmissionHowTo</a><br/>
<a href="http://www.htpcbeginner.com/install-transmission-web-interface-on-ubuntu-1204/">Install Transmission with web interface on Ubuntu</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[设置Server开机启动ssh服务]]></title>
    <link href="http://yaozh1918.github.io/14776396449421.html"/>
    <updated>2016-10-28T15:27:24+08:00</updated>
    <id>http://yaozh1918.github.io/14776396449421.html</id>
    <content type="html"><![CDATA[
<p>为了能够远程访问server，肯定需要通过ssh，但有时候服务器需要重启，而ssh server是在login后才启动的，也就是说我们需要坐在电脑前，手动登陆一次才能继续使用ssh。今天搜了一下，发现一个简单有效的<a href="http://askubuntu.com/questions/3913/start-ssh-server-on-boot">解决方案</a>。</p>

<span id="more"></span><!-- more -->

<p>其实核心就一条命令，这条命令的作用是设置ssh server在default runlevels时启动。</p>

<pre><code>sudo update-rc.d ssh defaults
</code></pre>

<p>但是此时，我们在图形界面下的Network Manager中修改的如ip地址，网关等是不会生效的。我们要在<code>/etc/network/interfaces</code>添加我们的设置。</p>

<pre><code>auto lo
iface lo inet loopback

auto enp4s0
iface enp4s0 inet static
address 111.111.111.111
netmask 255.255.254.0
gateway 111.111.111.1
dns-nameserver 8.8.8.8
</code></pre>

<p>具体的设置可以参考<a href="http://www.cyberciti.biz/faq/setting-up-an-network-interfaces-file/">这里</a>。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[构建一个可复用的Python实验环境(1)]]></title>
    <link href="http://yaozh1918.github.io/14770379590045.html"/>
    <updated>2016-10-21T16:19:19+08:00</updated>
    <id>http://yaozh1918.github.io/14770379590045.html</id>
    <content type="html"><![CDATA[
<p>毕竟是野路子出身，以前跑实验，总是每一个模型都有一套完整的代码，从读数据，跑模型，算得分，写代码的过程中也是大量复制粘贴，因为读数据和算得分的代码通常都是通用的。<br/>
这次访学，师兄拷给我一份实验环境的代码，看后惊叹原来代码要这样写，耦合解得非常好。但我这种强迫症总是觉得别人写的不漂亮，自己照着重写了一遍，结果并没能领悟其精髓，结果最后跑实验的时候各种出问题。项目好不容易结束，决定认认真真分析一下代码，再重新写一遍。代码在github上同步更新。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_0">目标</h2>

<p>我们的目标是写一个可复用的实验环境框架，这样拿到一个新的项目，只要对应的填入一些函数、类，就可以使用了。<br/>
那么这个实验环境要有怎样的特性呢？我们希望，在项目进行过程中，如果要添加一个数据集、模型、或是评估指标，只要简单的加入对应的文件就好，而不去影响其他代代码。<br/>
这样的一个环境搭好之后，就可以把它当成一个库来使用，编写额外的代码来调用其中的函数。</p>

<h2 id="toc_1">整体结构</h2>

<p>通常来说，一个完整的实验的流程是读取数据、用模型计算结果、对结果进行评估（打分），那么至少就要有三个基类：<code>dataset</code>,<code>model</code>,<code>metric</code>。此外，我们可能有不同的实验设置，例如80%拿来训练模型，20%作为测试集，这样还需要一个基类<code>setting</code>。最后我们再用一个<code>profile</code>类来把这些东西组装起来就好了。整体的结构就可以这样写：</p>

<pre><code>+explib
|-- +datasets
|-- +models
|-- +metrics
|-- +settings
|-- __init__.py
|-- base.py
|-- utils.py
</code></pre>

<p>我们将基类都定义在<code>base.py</code>中，<code>utils.py</code>放一些常用的小函数，例如io操作等。模型等具体实现写在对应的文件夹下。</p>

<h2 id="toc_2">基类定义</h2>

<p>首先定义整个实验环境的基类，其实没有也可以，这样写更一致一点。</p>

<pre><code>from abc import abstractmethod, ABCMeta
from bunch import Bunch


class expBase(object):
    __metaclass__ = ABCMeta

    def __init__(self):
        self.params = Bunch()
</code></pre>

<p>首先导入<code>abc</code>模块，这样通过声明<code>__metaclass__ = ABCMeta</code>表明当前的<code>class</code>是一个抽象类，无法被直接实例化。<code>expBase</code>继承自<code>object</code>，考虑到所有的子类都可能有参数，我们在<code>__init__</code>中初始化<code>self.params</code>为一个<code>Bunch</code>。<code>Bunch</code>是<code>dict</code>的一个子类，可以用<code>params.x1</code>来代替<code>params[&#39;x1&#39;]</code>，写起来比较方便。</p>

<h3 id="toc_3">dataset and model</h3>

<p>接下来定义关于数据集合模型的基类：</p>

<pre><code>class expDataset(expBase):
    __metaclass__ = ABCMeta

    def __init__(self):
        super(expDataset, self).__init__()

    @abstractmethod
    def load(self):
        &quot;&quot;&quot;Load data
        to be implemented in subclass
        &quot;&quot;&quot;
        return


class expModel(expBase):
    __metaclass__ = ABCMeta

    def __init__(self):
        super(expModel, self).__init__()

    @abstractmethod
    def fit(self, data):
        &quot;&quot;&quot;Fit model to the data
        to be implemented in subclass
        &quot;&quot;&quot;
        return
</code></pre>

<p>我们希望所有的dataset都有一个统一的<code>load</code>接口，可以读取数据并返回，所有的<code>model</code>也要有一个<code>fit</code>接口来将模型用到data上。通过用装饰器<code>abstractmethod</code>声明函数为抽象方法，子类中必须要实现该方法，否则会报错。</p>

<p>这里函数命名<code>fit</code>主要是参考scikit-learn的写法。</p>

<h3 id="toc_4">metric</h3>

<p>接下来定义评估指标的基类：</p>

<pre><code>class expMetric(expBase):
    __metaclass__ = ABCMeta

    def __init__(self):
        super(expMetric, self).__init__()
        self.values = []

    @abstractmethod
    def evaluate(self):
        &quot;&quot;&quot;Evaluate the result
        to be implemented in subclass
        &quot;&quot;&quot;
        return
</code></pre>

<p>其实这一块我还是不太清楚该怎么写，这是之前师兄的写法，这样写考虑的是一般实验都要重复多次取平均值，用一个列表来存每次的结果，这样最后可以返回均值和方差。</p>

<p>这个类有待改进。</p>

<h3 id="toc_5">setting</h3>

<p>实验设置的类：</p>

<pre><code>class expSetting(expBase):
    __metaclass__ = ABCMeta

    def __init__(self):
        super(expSetting, self).__init__()
        self.dataset = None
        self.model = None
        self.metrics = []

    def setup(self, dataset, model, metrics):
        self.dataset = dataset
        self.model = model
        self.metrics = metrics

    @abstractmethod
    def run(self):
        &quot;&quot;&quot;Fit data and evaluate the result
        to be implemented in subclass
        &quot;&quot;&quot;
        return
</code></pre>

<p>要注意的一点是，评估方法可能有多个，所以应该是一个列表。</p>

<h3 id="toc_6">profile</h3>

<p>最后一个类，这个类将4个模块进行拼装并运行：</p>

<pre><code>
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ubuntu安装Nvidia驱动及tensorflow]]></title>
    <link href="http://yaozh1918.github.io/14652179403403.html"/>
    <updated>2016-06-06T20:59:00+08:00</updated>
    <id>http://yaozh1918.github.io/14652179403403.html</id>
    <content type="html"><![CDATA[
<p>刚刚搞了台新机器，带的GTX960显卡，想装个ubuntu跑deep learning，在安装中遇到了一些问题，这里记录一下。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_0">安装Ubuntu</h2>

<p>用u盘安装<code>64bit 14.04LTS</code>，不知道是不是显示器的原因，960没有vga的口，所以用了个vga转dvi的转接头，导致安装引导一进图形界面显示器就黑屏，显示分辨率超出范围。</p>

<p>因此安装的时候要稍作处理，u盘一进引导就按<code>F1</code>，进入下图的这个界面，然后选择第一个试用或者第二个安装，按<code>F6</code>，可以看到一个参数<code>nomodeset</code>，回车选中，然后再回车开始。然而这个方法对我的电脑不管用，换用第二种方法，按<code>Tab</code>，在启动参数末尾删除<code>--</code>，再加上<code>nomodeset</code>，回车安装，此时分辨率会调整为<code>640x480</code>。</p>

<p><img src="http://o6jlkx4pl.bkt.clouddn.com/2016-06-06-IMG_1360%20copy.jpg" alt="IMG_1360 copy " class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p>

<p>系统装好后，应该还是进不去的，也要调为<code>nomodest</code>，或者简单一点，Grub时选择<code>Advanced-recovery mode-resume</code>。进系统安装nvidia驱动后应该就可以了。</p>

<h2 id="toc_1">安装Nvidia驱动</h2>

<p>安装nvidia驱动尝试了好久才装上去，一开始我是从官网上下载的驱动，安装没问题，但分辨率就被锁在<code>640x480</code>了，折腾了半天，后来发现了一个很方便的安装方法。</p>

<p>打开<code>Additional Drivers</code>，然后直接选中驱动，apply，重启即可。</p>

<p><img src="http://o6jlkx4pl.bkt.clouddn.com/2016-06-06-Screenshot%20from%202016-06-06%2019_30_39.png" alt="Additional Drivers " class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p>

<h2 id="toc_2">安装python</h2>

<p>图省事，我选择了直接安装<a href="https://www.continuum.io/downloads">Anaconda</a>，下载好<code>.sh</code>文件，我用的是2.7的版本，然后terminal执行：</p>

<pre><code>bash ~/Downloads/Anaconda2-4.0.0-Linux-x86_64.sh
</code></pre>

<p>根据提示完成安装。</p>

<p>完成后最好<code>source ~/.bashrc</code>一下。</p>

<h2 id="toc_3">安装Cuda</h2>

<p>首先从nvidia官网上下载<a href="https://developer.nvidia.com/cuda-downloads">Cuda Toolkit</a>和<a href="https://developer.nvidia.com/cudnn">cudnn</a>，其中cuDNN需要注册一个账号。</p>

<p>先安装cuda，我下载的是<code>deb</code>文件，然后安装：</p>

<pre><code>sudo dpkg -i cuda-repo-ubuntu1404-7-5-local_7.5-18_amd64.deb
sudo apt-get update
sudo apt-get install cuda
</code></pre>

<p>现在安装cudnn，按照tensorflow官网提示，安装v5要从源码安装tensorflow才行，懒得搞，直接下载v4版本的。然后复制文件到cuda的目录：</p>

<pre><code>tar xvzf cudnn-7.0-linux-x64-v4.0-prod.tgz
sudo cp cuda/include/* /usr/local/cuda/include
sudo cp cuda/lib64/* /usr/local/cuda/lib64
sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*
</code></pre>

<p>然后在<code>~/.bash_profile</code>中加入：</p>

<pre><code>export LD_LIBRARY_PATH=&quot;$LD_LIBRARY_PATH:/usr/local/cuda/lib64&quot;
export CUDA_HOME=/usr/local/cuda
</code></pre>

<p>最后再更新一下：<code>source .bash_profile</code>。</p>

<h2 id="toc_4">安装Tensorflow</h2>

<p>准备工作都已搞定，最后安装tensorflow即可，官网给出了anaconda环境的<a href="https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#anaconda-environment-installation">安装方法</a>，但我想把tensorflow安装到全局，而不是一个环境中，所以直接用pip安装即可。</p>

<p>（下午第一次装的时候还是0.8的教程，晚上写这篇文章的时候就更新到了0.9...）</p>

<p>找到gpu版本的地址：</p>

<pre><code>$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl
</code></pre>

<p>然后用<code>pip</code>安装，注意我们要用anaconda中的pip：</p>

<pre><code>$ sudo ~/anaconda2/bin/pip install --ignore-installed --upgrade $TF_BINARY_URL
</code></pre>

<p>其中，<code>--ignore-installed</code>是因为<code>easy_install</code>会报错。</p>

<h2 id="toc_5">Test</h2>

<p>至此，已全部安装好了！让我们来测试一下！</p>

<pre><code>python -c &#39;import tensorflow&#39;
</code></pre>

<p>看到全是successfully opened...就可以了！</p>

<p>也可以跑个cnn试一下：</p>

<pre><code>python -m tensorflow.models.image.mnist.convolutional
</code></pre>

<p>gpu就是快啊，我的rmbp每步是要290ms左右，现在只要12ms，不说了，跑cnn去了XD</p>

<h2 id="toc_6">Error</h2>

<p>2016-6-22更新：</p>

<p>有段时间没用，突然出现了这样的问题：</p>

<pre><code>libcudart.so.7.5: cannot open shared object file: No such file or directory
</code></pre>

<p>解决方案：</p>

<pre><code>sudo ldconfig /usr/local/cuda/lib64
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MongoDB安装及pymongo入门]]></title>
    <link href="http://yaozh1918.github.io/14641625300131.html"/>
    <updated>2016-05-25T15:48:50+08:00</updated>
    <id>http://yaozh1918.github.io/14641625300131.html</id>
    <content type="html"><![CDATA[
<p>在自己的ubuntu上装MongoDB时遇到了一些问题，装好也不太会用，写篇文章记录下吧！</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">安装</a>
</li>
<li>
<a href="#toc_1">启动/停止服务</a>
</li>
<li>
<a href="#toc_2">PyMongo</a>
<ul>
<li>
<a href="#toc_3">Installation</a>
</li>
<li>
<a href="#toc_4">Tutorial</a>
<ul>
<li>
<a href="#toc_5">Making a connection</a>
</li>
<li>
<a href="#toc_6">Database</a>
</li>
<li>
<a href="#toc_7">Collection</a>
</li>
<li>
<a href="#toc_8">Documents</a>
<ul>
<li>
<a href="#toc_9">insert</a>
</li>
<li>
<a href="#toc_10">find</a>
</li>
<li>
<a href="#toc_11">sort</a>
</li>
<li>
<a href="#toc_12">update</a>
</li>
<li>
<a href="#toc_13">remove</a>
</li>
</ul>
</li>
<li>
<a href="#toc_14">Aggregation</a>
</li>
<li>
<a href="#toc_15">Indexes</a>
</li>
</ul>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">安装</h2>

<p>我的系统是<code>Ubuntu 14.04.4 LTS</code>.</p>

<p>安装还是按照<a href="https://docs.mongodb.com/manual/tutorial/install-mongodb-on-ubuntu/">官方教程</a>比较好，虽然<code>ubuntu</code>的<code>apt-get</code>中有个<code>mongodb</code>，但我们用的不是这个。</p>

<p>安装过程也比较无脑，照着做就好了：</p>

<pre><code>sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv EA312927
echo &quot;deb http://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/3.2 multiverse&quot; | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list
sudo apt-get update
sudo apt-get install -y mongodb-org
</code></pre>

<p>注意第一句是要导入一个<code>GPG Key</code>，不同版本不一样，实际装的时候去官网看一眼。</p>

<h2 id="toc_1">启动/停止服务</h2>

<p>安装好后就可以启动停止mongodb的服务了：</p>

<pre><code>sudo service mongod start
sudo service mongod stop
sudo service mongod restart
</code></pre>

<p>注意这里的<code>mongod</code>指代的是mongodb的deamon。</p>

<h2 id="toc_2">PyMongo</h2>

<p>感觉shell不是很给力，就直接看pymongo的教程吧，下面是按照<a href="https://docs.mongodb.com/getting-started/python/">官网</a>的教程，和api的<a href="http://api.mongodb.com/python/current/index.html">文档</a>整理写的一些基本函数。</p>

<p>首先导入官网给出的一个数据，我们后面的教程都会在这个数据上操作：</p>

<pre><code>wget https://raw.githubusercontent.com/mongodb/docs-assets/primer-dataset/primer-dataset.json
mongoimport --db test --collection restaurants --drop --file primer-dataset.json
</code></pre>

<p>第一行是下载文件，第二行是将数据导入到名为test的database中名为restaurants的collection中。</p>

<h3 id="toc_3">Installation</h3>

<p>安装就用<code>pip</code>：</p>

<pre><code>sudo pip install pymongo
</code></pre>

<h3 id="toc_4">Tutorial</h3>

<p>首先导入module：</p>

<pre><code>from pymongo import MongoClient
</code></pre>

<h4 id="toc_5">Making a connection</h4>

<p>利用<code>MongoClient</code>创建一个cursor，参数是可以为空的，默认会连接到<code>localhost:27017</code>，也就是说以下三句效果相同：</p>

<pre><code>client = MongoClient()
client = MongoClient(&#39;localhost&#39;, 27017)
client = MongoClient(&#39;mongodb://localhost:27017/&#39;)
</code></pre>

<h4 id="toc_6">Database</h4>

<p>创建好连接后就可以对数据库进行操作了，例如想看所有的数据库：</p>

<pre><code>client.database_names()
</code></pre>

<p>一般来说会看到一个叫<code>test</code>的数据库。</p>

<p>连接到一个数据库有两种方法，第二种方法类似dict，如果数据库名有特殊字符就只能用这种。</p>

<pre><code>db = client.test
db = client[&#39;a-database&#39;]
</code></pre>

<p>注意，MongoDB<strong>无需显式创建数据库</strong>，例如上面的代码，<code>a-database</code>如果不存在，此时会自动创建一个（其实也不是，只有真正对这个数据库进行操作后才会创建）。</p>

<p>如果要删除一个database，则需要在<code>client</code>中操作：</p>

<pre><code>db.drop_database(&#39;a_database&#39;)
</code></pre>

<p>现在我们进入<code>test</code>数据库进行后续的教程。</p>

<h4 id="toc_7">Collection</h4>

<p>因为<code>MongoDB</code>是<code>NoSQL</code>的，有一些概念与传统的RDBMS有区别，比如这个<code>collection</code>就类似于RDBMS中的关系表<code>table</code>，<code>table</code>是记录<code>row</code>的集合，而<code>collection</code>是<code>document</code>的集合。</p>

<p>和操作database很像，两种连接方式，无须显式创建。</p>

<pre><code>db.collection_names()  # 查看当前数据库下的所有collection
collection = db.restaurants
collection = db[&#39;restaurants&#39;]
</code></pre>

<p>类似的，删除<code>collection</code>要在database中进行：</p>

<pre><code>db.drop_collection(&#39;a_collection&#39;)
</code></pre>

<h4 id="toc_8">Documents</h4>

<p><code>MongoDB</code>中的数据都是以一种类似<code>JSON</code>的文档<code>document</code>存储的，类似RDBMS中的记录／行<code>row</code>，不同的是，一个collection中的documents可以有不同的域<code>field</code>，而一张表中的行必须有相同的属性／列<code>column</code>。</p>

<p>document按照类<code>JSON</code>的格式存，在<code>python</code>中就会表示为<code>dict</code>，例如下面就是一个document的内容：</p>

<pre><code>from datetime import datetime
d = {
        &quot;address&quot;: {
            &quot;street&quot;: &quot;2 Avenue&quot;,
            &quot;zipcode&quot;: &quot;10075&quot;,
            &quot;building&quot;: &quot;1480&quot;,
            &quot;coord&quot;: [-73.9557413, 40.7720266]
        },
        &quot;borough&quot;: &quot;Manhattan&quot;,
        &quot;cuisine&quot;: &quot;Italian&quot;,
        &quot;grades&quot;: [
            {
                &quot;date&quot;: datetime.strptime(&quot;2014-10-01&quot;, &quot;%Y-%m-%d&quot;),
                &quot;grade&quot;: &quot;A&quot;,
                &quot;score&quot;: 11
            },
            {
                &quot;date&quot;: datetime.strptime(&quot;2014-01-16&quot;, &quot;%Y-%m-%d&quot;),
                &quot;grade&quot;: &quot;B&quot;,
                &quot;score&quot;: 17
            }
        ],
        &quot;name&quot;: &quot;Vella&quot;,
        &quot;restaurant_id&quot;: &quot;41704620&quot;
    }
)
</code></pre>

<p>假设我们现在的collection就是一开始添加的<code>restaurants</code>，可以先看看有多少documents：</p>

<pre><code>collection.count()
</code></pre>

<h5 id="toc_9">insert</h5>

<p>现在将上面的那个document插入：</p>

<pre><code>result = collection.insert_one(d)
result.inserted_id
</code></pre>

<p>注意会返回一个<code>InsertOneResult</code>类型的数据，其有个属性<code>inserted_id</code>会给出插入的document在collection中的id号，对于一个collection中各个documents都是不同的，如果插入时没有给定，则会自动创建。</p>

<p>如果想一次插入多个，可以用<code>insert_many</code>：</p>

<pre><code>result = collection.insert_many([d1, d2, d3])
result.inserted_ids
</code></pre>

<p>类似的，会返回一个<code>InsertManyResult</code>类的数据，可以看所有documents的id。</p>

<h5 id="toc_10">find</h5>

<p>本小节会介绍两个函数<code>find</code>和<code>find_one</code>，从函数名就可以猜出，<code>find</code>会找出所有符合条件的documents，其返回的数据类型为<code>Cursor</code>，实际上就是一个<code>iterator</code>，需要迭代访问；而<code>find_one</code>只会返回第一个满足条件的document，数据类型是<code>dict</code>。</p>

<pre><code>cursor = find()  # 条件为空，也就是会返回所有的documents
for doc in cursor:  # 迭代访问
    print doc
find_one()  # 返回第一个document
</code></pre>

<p>还是拿<code>restaurants</code>做示范，如果要找<code>borough</code><strong>等于</strong><code>Manhattan</code>的，将条件按<code>dict</code>格式传进去：</p>

<pre><code>cursor = db.restaurants.find({&quot;borough&quot;: &quot;Manhattan&quot;})
</code></pre>

<p><code>dict</code>可以嵌套<code>dict</code>，同样的<code>document</code>也可以嵌套<code>document</code>，例如<code>address</code>下还有<code>street</code>、<code>zipcode</code>等，如果要找<code>zipcode</code>等于<code>10075</code>的，则要用<code>.</code>来连接：</p>

<pre><code>cursor = db.restaurants.find({&quot;address.zipcode&quot;: &quot;10075&quot;})
</code></pre>

<p>一家餐厅可能有多个用户打分，这就形成了一个documents的列表，<code>grades</code>就是这样，如果想找<code>grades</code>中是否<strong>包含</strong>一个document的<code>grade</code><strong>等于</strong><code>B</code>，则可以这样实现：</p>

<pre><code>cursor = db.restaurants.find({&quot;grades.grade&quot;: &quot;B&quot;})
</code></pre>

<p>如果涉及到操作符，如大于小于，则用下面这种格式（dict套dict）：</p>

<pre><code>{ &lt;field1&gt;: { &lt;operator1&gt;: &lt;value1&gt; } }
</code></pre>

<p>比如</p>

<pre><code>cursor = db.restaurants.find({&quot;grades.score&quot;: {&quot;$gt&quot;: 30}})
</code></pre>

<p>具体的操作符可以去看<a href="https://docs.mongodb.com/manual/reference/operator/">官方文档</a>。</p>

<p><code>AND</code>操作很简单，只要将多个条件写在dict中就好：</p>

<pre><code>cursor = db.restaurants.find({&quot;cuisine&quot;: &quot;Italian&quot;, &quot;address.zipcode&quot;: &quot;10075&quot;})
</code></pre>

<p><code>OR</code>则比较特殊，dict中用<code>$or</code>作为key，条件写在list中作value：</p>

<pre><code>cursor = db.restaurants.find(
    {&quot;$or&quot;: [{&quot;cuisine&quot;: &quot;Italian&quot;}, {&quot;address.zipcode&quot;: &quot;10075&quot;}]})
</code></pre>

<p><code>find_one</code>与上面类似，就不多说了，唯一的区别就是返回的是一个dict。</p>

<h5 id="toc_11">sort</h5>

<p>查询到多个结果后可能希望对结果排序，那么就要对<code>Cursor</code>用<code>sort</code>，比如希望先按<code>borough</code>升序，再按<code>zipcode</code>升序，可以写为：</p>

<pre><code>cursor = db.restaurants.find().sort([
    (&quot;borough&quot;, pymongo.ASCENDING),
    (&quot;address.zipcode&quot;, pymongo.ASCENDING)
])
</code></pre>

<p>如果只按照一个属性排序，则直接将属性和方向传入即可：</p>

<pre><code>cursor = db.restaurants.find().sort(&quot;borough&quot;, pymongo.ASCENDING)
</code></pre>

<p>注意，<code>sort</code>是inplace的，虽然也会返回一个<code>Cursor</code>，但假如已经查询完返回了一个<code>Cursor</code>，则下面两句代码没有区别：</p>

<pre><code>cursor = cursor.sort(&quot;borough&quot;, pymongo.ASCENDING)
cursor.sort(&quot;borough&quot;, pymongo.ASCENDING)
</code></pre>

<h5 id="toc_12">update</h5>

<p>类似的，本节也有两个函数<code>update_one</code>和<code>update_many</code>。</p>

<p>只需将查询条件和更新内容作为两个参数传入即可，例如修改<code>cuisine</code>为<code>American (New)</code>（用到<code>$set</code>），修改<code>lastModified</code>为当前时间（用到<code>$currentDate</code>）：</p>

<pre><code>result = db.restaurants.update_one(
    {&quot;name&quot;: &quot;Juni&quot;},
    {
        &quot;$set&quot;: {
            &quot;cuisine&quot;: &quot;American (New)&quot;
        },
        &quot;$currentDate&quot;: {&quot;lastModified&quot;: True}
    }
)
result.matched_count  # 查找到的数量
result.modified_count  # 实际修改的数量
</code></pre>

<p>这个部分也有好多操作符，还是推荐去扫一遍<a href="https://docs.mongodb.com/manual/reference/operator/">官方文档</a>。</p>

<p>这个是更新，如果要进行替换，可以用<code>replace_one</code>（并没有<code>replace_many</code>），第一个参数为条件，第二个参数为替换内容，替换后只有<code>_id</code>会保留。</p>

<h5 id="toc_13">remove</h5>

<p>要删除document也有两个函数，<code>delete_one</code>和<code>delete_many</code>，只需传入查询条件即可，返回的结果可以查看实际删除的数量：</p>

<pre><code>result = db.restaurants.delete_many({&quot;borough&quot;: &quot;Manhattan&quot;})
result.deleted_count
</code></pre>

<p>如果要删除所有，则可以传入条件<code>{}</code>（为什么不drop掉collection呢？）</p>

<h4 id="toc_14">Aggregation</h4>

<p>聚合这一块内容很复杂，通过<code>aggregate</code>可以实现SQL中的<code>group by</code>、<code>having</code>等功能，在3.2版本后，还可以实现<code>join</code>操作！坑比较大，等有空再单独写一章吧。</p>

<h4 id="toc_15">Indexes</h4>

<p><code>index</code>主要是用来加速特定查找的，如果没有索引，每次查找就要遍历一遍collection，效率自然不高。<code>MongoDB</code>会自动把索引建在<code>_id</code>上，通常对我们的查找没有帮助，需要我们手动选择一个field建立，当然也可以选择一组fields建立：</p>

<pre><code>db.restaurants.create_index([
    (&quot;cuisine&quot;, pymongo.ASCENDING),
    (&quot;address.zipcode&quot;, pymongo.DESCENDING)
])
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Installing Pygame on Mac OS X]]></title>
    <link href="http://yaozh1918.github.io/14623285310444.html"/>
    <updated>2016-05-04T10:22:11+08:00</updated>
    <id>http://yaozh1918.github.io/14623285310444.html</id>
    <content type="html"><![CDATA[
<p>It&#39;s said that <code>pygame</code> provided on their <a href="http://www.pygame.org/download.shtml">website</a> is not compatible with Mac system python because <code>pygame</code> is built for 32 bit python while system python is 64 bit. </p>

<span id="more"></span><!-- more -->

<p>Luckily I have found a <a href="https://bitbucket.org/pygame/pygame/issues/82/homebrew-on-leopard-fails-to-install#comment-627494">solution</a> and now I post it on my blog in case I forget it.</p>

<pre><code>brew install mercurial
brew install sdl sdl_image sdl_mixer sdl_ttf smpeg portmidi 
sudo pip install hg+http://bitbucket.org/pygame/pygame
</code></pre>

<p>After installation, you can validate it via this interesting <a href="https://github.com/sourabhv/FlapPyBird">repo</a>.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sort 3x3 grid by rotating 2x2 subgrids]]></title>
    <link href="http://yaozh1918.github.io/14621764503901.html"/>
    <updated>2016-05-02T16:07:30+08:00</updated>
    <id>http://yaozh1918.github.io/14621764503901.html</id>
    <content type="html"><![CDATA[
<p>Given a 3x3 grid filled with 1-9, how do you sort it to another given grid by rotating 2x2 subgrids?</p>

<p>Some discussions can be found <a href="http://stackoverflow.com/questions/23433442/sort-3x3-grid-by-rotating-2x2-subgrids">here</a>.</p>

<span id="more"></span><!-- more -->

<pre><code>import numpy as np
from Queue import Queue


class Sort_Grid:

    def __init__(self, S=123456789, T=947852361):
        self.source = S
        self.terminal = T

    def num2array(self, num):
        ret = np.zeros(9, np.uint8)
        for i in xrange(9):
            ret[i] = num / 10**(8-i)
            num -= ret[i] * 10**(8-i)
        return ret

    def array2num(self, arr):
        return np.sum(arr * 10**np.arange(8,-1,-1))

    def biBFS(self):
        self.basis = [[0,0], [0,1], [1,0], [1,1]]
        self.subbasis = [[1,3,0,2], [2,0,3,1]]
        self.lookup = [np.zeros(987654321+1, bool) for i in range(2)]
        self.queue = [Queue() for i in range(2)]
        self.lookup[0][self.source] = True
        self.lookup[1][self.terminal] = True
        level0 = 0
        level1 = 0
        c0 = self.source
        c1 = self.terminal
        if c0 == c1:
            return 0, 0, 0
        while True:
            if level0 &lt; level1:
                midstate = self.EnQueue(c0, level0, 0)
                if midstate:
                    return midstate, level0, level1
                c0, level0 = self.queue[0].get()
            else:
                midstate = self.EnQueue(c1, level1, 1)
                if midstate:
                    return midstate, level0, level1
                c1, level1 = self.queue[1].get()

    def EnQueue(self, state, level, ind):
        array = self.num2array(state)
        for i in range(4):
            shadow = array.copy()
            x, y = self.basis[i]
            base = np.array([x*3+y, x*3+y+1, (x+1)*3+y, (x+1)*3+y+1])
            for j in range(2):
                shadow[base] = array[base[self.subbasis[j]]]
                num = self.array2num(shadow)
                if ~self.lookup[ind][num]:
                    if self.lookup[1-ind][num]:
                        return num
                    self.queue[ind].put((num, level+1))
                    self.lookup[ind][num] = True

def biBFS(S, T, seqlist):
    if S == T:
        return
    mid, l0, l1 = Sort_Grid(S, T).biBFS()
    if mid==0 or mid in seqlist:
        return
    seqlist.insert(seqlist.index(S)+1, mid)
    biBFS(S, mid, seqlist)
    biBFS(mid, T, seqlist)

def main():
    S = 123456789
    T = 947852361
    seqlist = [S, T]
    biBFS(S, T, seqlist)
    return seqlist

if __name__ == &#39;__main__&#39;:
    main()

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[开博客啦！]]></title>
    <link href="http://yaozh1918.github.io/14621706709086.html"/>
    <updated>2016-05-02T14:31:10+08:00</updated>
    <id>http://yaozh1918.github.io/14621706709086.html</id>
    <content type="html"><![CDATA[
<p>终于开了一个博客，感谢有<code>MWeb</code>这么好用的软件。</p>

<p>之前一直用的作业部落的<code>Cmd Markdown</code>，也很不错，也感谢一下XD。但比起<code>MWeb</code>还是逊色不少，这个可以直接生成静态页面，几步就可以挂到<code>Github Pages</code>上了，方便的很！图片、URL什么的插入都很方便。安利一波！</p>

<span id="more"></span><!-- more -->

<p>那么这个博客就主要记录一下关于Machine Learning, Data Mining的一些内容吧，可能还会有一些基本的算法、编程语言、数学的内容。</p>

<p>接下来的内容仅仅是测试一下各项功能是否正常。</p>

<ul>
<li>
<a href="#toc_0">LaTex</a>
</li>
<li>
<a href="#toc_1">Sequence and Flow Chart</a>
</li>
<li>
<a href="#toc_2">Images</a>
</li>
</ul>


<h2 id="toc_0">LaTex</h2>

<p>For example this is a Block level \[x = {-b \pm \sqrt{b^2-4ac} \over 2a}\] formula, and this is an inline Level \(x = {-b \pm \sqrt{b^2-4ac} \over 2a}\) formula.</p>

<p>\[ \frac{1}{\Bigl(\sqrt{\phi \sqrt{5}}-\phi\Bigr) e^{\frac25 \pi}} =<br/>
1+\frac{e^{-2\pi}} {1+\frac{e^{-4\pi}} {1+\frac{e^{-6\pi}}<br/>
{1+\frac{e^{-8\pi}} {1+\ldots} } } } \]</p>

<h2 id="toc_1">Sequence and Flow Chart</h2>

<pre><code class="language-sequence">Andrew-&gt;China: Says Hello
Note right of China: China thinks about it
China--&gt;Andrew: How are you?
Andrew-&gt;&gt;China: I am good thanks!
</code></pre>

<pre><code class="language-flow">st=&gt;start: Start:&gt;http://www.google.com[blank]
e=&gt;end:&gt;http://www.google.com
op1=&gt;operation: My Operation
sub1=&gt;subroutine: My Subroutine
cond=&gt;condition: Yes
or No?:&gt;http://www.google.com
io=&gt;inputoutput: catch something...

st-&gt;op1-&gt;cond
cond(yes)-&gt;io-&gt;e
cond(no)-&gt;sub1(right)-&gt;op1
</code></pre>

<h2 id="toc_2">Images</h2>

<p>My avatar.</p>

<p><img src="http://ooo.0o0.ooo/2016/05/02/5726ffee18657.jpg" alt="icon"/></p>

<p>Resize and center.</p>

<p><img src="http://ooo.0o0.ooo/2016/05/02/5726ffee18657.jpg" alt="icon" class="mw_img_center" style="width:100px;display: block; clear:both; margin: 0 auto;"/></p>

]]></content>
  </entry>
  
</feed>
