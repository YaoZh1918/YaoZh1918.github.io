<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  Python实现人工神经网络(2) - YaoZh1918
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="YaoZh1918" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:yaozh1918.github.io ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; YaoZh1918</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
       
       <li><a href="index.html">HOME</a></li>
    <li><a href="archives.html">Archives</a></li>
    <li><a href="about.html">ABOUT</a></li>

    <li><label>Categories</label></li>

        
            <li><a href="miscellaneous.html">Miscellaneous</a></li>
        
            <li><a href="algorithm.html">Algorithm</a></li>
        
            <li><a href="python.html">Python</a></li>
        
            <li><a href="linux.html">Linux</a></li>
        
            <li><a href="Math.html">Math</a></li>
        
            <li><a href="ml.html">Machine Learning</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
  $(function(){
    $('#menu_item_index').addClass('is_active');
  });
</script>
<div class="row">
  <div class="large-8 medium-8 columns">
      <div class="markdown-body article-wrap">
       <div class="article">
          
          <h1>Python实现人工神经网络(2)</h1>
     
        <div class="read-more clearfix">
          <span class="date">2017/7/19</span>

          <span>posted in&nbsp;</span> 
          
              <span class="posted-in"><a href='ml.html'>Machine Learning</a></span>
           
         
          <span class="comments">
            

            
              &nbsp;<a  class="ds-thread-count" data-thread-key="15004749480094.html" data-count-type="comments" href="15004749480094.html#ds-thread"></a>
            
          </span>

        </div>
      </div><!-- article -->

      <div class="article-content">
      <p>在上一篇<a href="https://yaozh1918.github.io/15000325978554.html">Python实现人工神经网络(1)</a>文章中，我们体验了计算图与BP的思想，不过当时的结点只能处理标量计算，今天，我们实现下能处理基本矩阵运算的计算图，并用它实现一个线性回归。库代码见<a href="https://github.com/YaoZh1918/PyNeuralNet/tree/master/neuralnet_v1">repo</a>。</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Graph</a>
</li>
<li>
<a href="#toc_1">Node</a>
<ul>
<li>
<a href="#toc_2">Input与Variable</a>
</li>
<li>
<a href="#toc_3">Op运算符</a>
<ul>
<li>
<a href="#toc_4">矩阵乘法Matmul</a>
</li>
<li>
<a href="#toc_5">需要Broadcasting的运算</a>
</li>
<li>
<a href="#toc_6">Reduce</a>
</li>
<li>
<a href="#toc_7">其他运算符</a>
</li>
</ul>
</li>
<li>
<a href="#toc_8">重载运算符</a>
</li>
</ul>
</li>
<li>
<a href="#toc_9">实战：线性回归</a>
<ul>
<li>
<a href="#toc_10">生成数据</a>
</li>
<li>
<a href="#toc_11">定义计算图</a>
</li>
<li>
<a href="#toc_12">优化</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">Graph</h2>

<p>上次我们用一个列表维护了计算图的拓扑排序，这次我们委托给一个专门的Graph对象，类似于Tensorflow的Graph和Session的结合。利用Graph管理结点，以后也方便拓展功能。</p>

<p>这里讲一下主要的几个函数，完整的代码可见repo。</p>

<p>首先依赖的模块有：</p>

<pre><code>import networkx as nx
from matplotlib import pyplot as plt
from collections import Counter
import contextlib
</code></pre>

<p>可以猜到，我们实际上用一个<code>networkx.DiGraph</code>有向图对象来协助我们管理我们的计算图，这样我们还可以可视化我们的计算图：</p>

<pre><code>class Graph:
    &quot;&quot;&quot;Computation Graph&quot;&quot;&quot;

    def __init__(self):
        self._g = nx.DiGraph()
        self._name_counter = Counter()
        self.feed_dict = None
    
    def add(self, ins, node):
        &quot;&quot;&quot;
        Add the new node and edges to the graph.
        :param ins: a list of nodes
        :param node: the new node
        &quot;&quot;&quot;
        self._check(ins, node.name)
        self._g.add_node(node, name=node.name)
        for in_node in ins:
            self._g.add_edge(in_node, node)
    
    # ... other methods ...
</code></pre>

<p><code>__init__</code>中，我们实例化了一个<code>DiGraph</code>，添加结点的操作<code>add</code>实际上就是添加到这个有向图中；<code>Counter</code>对象是用来管理结点名字的，不是重点，就跳过啦；<code>feed_dict</code>是为计算图提供输入的，类似tensorflow中的feed_dict，不过用法稍有不同，后面会看到。</p>

<p>接下来是前向和后向传播的两个函数：</p>

<pre><code>class Graph:
    def forward(self):
        &quot;&quot;&quot;forward pass through entire graph&quot;&quot;&quot;
        for node in self.topological():
            node.eval()

    def backward(self, start_from=None):
        &quot;&quot;&quot;Backward Pass (BackPropagation)&quot;&quot;&quot;
        topo_order = self.topological()
        if start_from is None:
            start_from = topo_order[-1]
        start_from._dout += 1.0
        for node in reversed(topo_order):
            node.propagate()
    
    # ...
</code></pre>

<p><code>forward</code>即前向传播，主要就是按照拓扑排序计算了每个结点的值；<code>backward</code>后向传播，按照拓扑排序逆向传播梯度，有一个参数start_from，即梯度从哪一个结点开始传播，将其上游梯度设为<code>1.0</code>，原因在上一篇文章中也解释过了，默认是最后一个结点。现在只是一个粗略的版本，在后续的文章中，我们会稍微优化这两个方法。</p>

<p>接下来是两个非常关键的上下文管理器：</p>

<pre><code>class Graph:
    @contextlib.contextmanager
    def as_default(self):
        &quot;&quot;&quot;Replace the default graph with the current graph.&quot;&quot;&quot;
        backup_g = DEFAULTS[&#39;graph&#39;]
        DEFAULTS[&#39;graph&#39;] = self
        try:
            yield self
        except Exception:
            raise
        finally:
            DEFAULTS[&#39;graph&#39;] = backup_g

    @contextlib.contextmanager
    def one_pass(self, feed_dict=None):
        &quot;&quot;&quot;
        context manager
        :param feed_dict: a dict whose keys are Nodes.
        &quot;&quot;&quot;
        self.feed_dict = feed_dict
        try:
            yield
        except Exception:
            raise
        finally:
            self.feed_dict = None
            self.reset_nodes()
    
    # ...

DEFAULTS = {&#39;graph&#39;: Graph()}

def get_default_graph():
    &quot;&quot;&quot;
    Get default(current) graph.
    &quot;&quot;&quot;
    return DEFAULTS[&#39;graph&#39;]
</code></pre>

<p>首先是<code>as_default</code>，看名字就知道是仿照tensorflow的XD，用法也基本一致。引入这个可以简化代码，新建结点<code>Node</code>对象时，不需要显式传入<code>Graph</code>，结点会自己调用<code>get_default_graph</code>方法，这样，直接一个<code>with</code>，把构建代码写下面就好。</p>

<p>另一个<code>one_pass</code>是在一轮迭代中用的，可以传入<code>feed_dict</code>（即mini batch），获取数据，并最后重置结点（清空缓存和梯度）。在后面的实战环节，可以看到这两个上下文管理器的具体用法。</p>

<p>还有一些其他的方法，比如画图、返回所有结点、返回所有可更新结点，不是重点，就先跳过了。整个文件的结构大概就是这样：</p>

<pre><code>&quot;&quot;&quot;graph.py&quot;&quot;&quot;

DEFAULTS = {&#39;graph&#39;: None}
__all__ = [&#39;Graph&#39;, &#39;get_default_graph&#39;]

class Graph:
    &quot;&quot;&quot;Computation Graph&quot;&quot;&quot;
    # ...        

DEFAULTS = {&#39;graph&#39;: Graph()}

def get_default_graph():
    &quot;&quot;&quot;
    Get default(current) graph.
    &quot;&quot;&quot;
    return DEFAULTS[&#39;graph&#39;]
</code></pre>

<h2 id="toc_1">Node</h2>

<p><code>Node</code>实例化时，只需将自己加入到对应的计算图中就好，<code>Graph</code>对象用<code>get_default_graph</code>获得。</p>

<pre><code>import numpy as np
import abc
from neuralnet_v1 import get_default_graph

class Node(abc.ABC):
    &quot;&quot;&quot;Base Class&quot;&quot;&quot;

    def __init__(self, ins=None, name=None, updatable=False):
        &quot;&quot;&quot;
        init method
        :param ins: a list of Node instances (dependent nodes).
        :param name: the name of the node
        :param updatable: whether the node is updatable
        &quot;&quot;&quot;
        self._ins = ins if ins else []
        self._g = get_default_graph()
        self._name = name
        self._updatable = updatable
        self._reset()
        # Make sure everything is okay, then add node to the graph.
        self._g.add(self._ins, self)
        
    def _reset(self):
        &quot;&quot;&quot;Clear cache and reset upstream gradient.&quot;&quot;&quot;
        self._cache = None
        self._dout = np.zeros(self.shape, dtype=DEFAULTS[&#39;dtype&#39;])
        
    # ...
</code></pre>

<p><code>updatable</code>表明当前结点是否可更新，在优化的时候我们只需修改可更新的结点，<code>_reset</code>即清空缓存（见下文）并设置上游梯度为0，梯度的<code>shape</code>和本身的<code>shape</code>一致。</p>

<p>这里补充一句，我们具体的计算全部委托给了<code>numpy</code>，在<code>numpy</code>中，n维数组的<code>shape</code>是一个长度为n的元组，标量的<code>shape</code>是<code>()</code>空元组，因此我们只要统一全部用<code>numpy</code>的数据类型，这里的最后一行代码就可以兼容标量运算。</p>

<p>结点需要提供两个方法来支持前向后向传播：</p>

<pre><code>class Node(abc.ABC):    
    def eval(self):
        &quot;&quot;&quot;Evaluate the current node (cached).&quot;&quot;&quot;
        if self._cache is None:
            self._cache = self._eval()
        return self._cache
    
    @abc.abstractmethod
    def _eval(self):
        &quot;&quot;&quot;Evaluate the current node.&quot;&quot;&quot;

    def send(self, dout):
        &quot;&quot;&quot;Receive upstream gradients.&quot;&quot;&quot;
        self._dout += dout

    @abc.abstractmethod
    def propagate(self):
        &quot;&quot;&quot;Evaluate and propagate the gradient.&quot;&quot;&quot;
    
    # ...
</code></pre>

<p>结合上面<code>Graph</code>的<code>forward</code><code>backwark</code>方法代码一起看，在前向传播时就调用<code>eval</code>方法，为了防止重复计算，我们添加了缓存；后向传播时，调用<code>propagate</code>传播梯度；<code>send</code>方法是用来接收梯度的，原因上次讲过了：涉及到参数共享时，我们希望将梯度汇总完再进行传播。</p>

<p>这里有个很尴尬的命名问题，<code>send</code>方法实际上是接收梯度的，但名字却是send，这里主要参考了协程中的<code>send</code>命名。</p>

<p>这样，在子类中，我们只需实现<code>_eval</code>和<code>propagate</code>方法就好了。</p>

<h3 id="toc_2">Input与Variable</h3>

<p>计算图中一开始的数据哪里来？靠用户传入和变量，这里我们实现了2个子类：<code>Input</code>用于接收数据，例如传入mini batch；<code>Variable</code>用于模型参数和常量（通过<code>updatable</code>区分）。</p>

<pre><code>class Input(Node):
    &quot;&quot;&quot;Input Node&quot;&quot;&quot;

    def __init__(self, shape, name=None):
        &quot;&quot;&quot;
        init method
        :param shape: a tuple representing the shape of the input
        :param name:  the name of the node
        &quot;&quot;&quot;
        self._shape = shape
        super().__init__(name=name, updatable=False)

    def _eval(self):
        if self.graph.feed_dict is None:
            raise ValueError(&#39;Please provide the value of &quot;{!r}&quot;&#39;.format(self))
        return np.array(self.graph.feed_dict[self], dtype=DEFAULTS[&#39;dtype&#39;])

    def propagate(self):
        pass


class Variable(Node):
    &quot;&quot;&quot;Variable Node&quot;&quot;&quot;

    def __init__(self, init_val, name=None, updatable=True):
        &quot;&quot;&quot;
        init method
        :param init_val: initial value
        :param name: the name of the node
        :param updatable: whether the node represents a constant
        &quot;&quot;&quot;
        self._value = np.array(init_val, dtype=DEFAULTS[&#39;dtype&#39;])
        self._shape = self._value.shape
        super().__init__(name=name, updatable=updatable)

    @property
    def value(self):
        return self._value

    def _eval(self):
        return self._value

    def propagate(self):
        pass
</code></pre>

<p>实例化<code>Input</code>时，只需指定<code>shape</code>，且不依赖于任何结点，不可更新。前向传播时，从所属的<code>Graph</code>中的<code>feed_dict</code>寻找数据，<code>np.array</code>一来复制一份数据，防止奇怪的bug，二来将标量转换成<code>shape=()</code>的数组，方便后面的处理。</p>

<p>实例化<code>Variable</code>则需要提供初始值，<code>shape</code>则直接从初始值中获取。这两种结点都不依赖于其他结点，因此它们在反向传播时，什么都不需要做。</p>

<h3 id="toc_3">Op运算符</h3>

<p><code>Op</code>运算符，既然是运算，肯定要有输入，运算结果不可直接更新，而且运算结果的<code>shape</code>往往也是可以提前推断的：</p>

<pre><code>class Op(Node):
    &quot;&quot;&quot;Operator&quot;&quot;&quot;

    def __init__(self, ins, name=None):
        ins = [n if isinstance(n, Node) else Variable(n, updatable=False) for n in ins]
        self._shape = self._infer_shape(*ins)
        super().__init__(ins=ins, name=name, updatable=False)

    @abc.abstractmethod
    def _infer_shape(self, *ins):
        &quot;&quot;&quot;Infer node shape from inputs.&quot;&quot;&quot;
</code></pre>

<p>因此，我们的运算符还需要额外实现一个<code>_infer_shape</code>方法。在<code>__init__</code>中，我们将所有的不是<code>Node</code>的输入先转换成了不可更新的<code>Variable</code>充当常量，这样我们可以直接将<code>Node</code>和<code>np.ndarray</code>进行运算。</p>

<p>还有一点，我们的<code>super</code>都是写在最后的，这是因为基类<code>Node</code>的<code>__init__</code>会将结点添加进<code>Graph</code>中，我们希望一切安排妥当后再添加，防止添加进一个没用的结点。</p>

<h4 id="toc_4">矩阵乘法Matmul</h4>

<pre><code>class Matmul(Op):
    &quot;&quot;&quot;Matrix Multiplication&quot;&quot;&quot;

    def __init__(self, a, b, name=None):
        &quot;&quot;&quot;
        init method
        :param a: a Node or np.ndarray with shape=(m, n)
        :param b: a Node or np.ndarray with shape=(n, p)
        :param name: the name of the node 
        &quot;&quot;&quot;
        super().__init__(ins=[a, b], name=name)

    def _infer_shape(self, a, b):
        shape_a = a.shape
        shape_b = b.shape
        if len(shape_a) != 2 or len(shape_b) != 2:
            raise ValueError(&#39;Inputs of &quot;Matmul&quot; should be matrices.&#39;)
        if shape_a[1] != shape_b[0]:
            raise ValueError(&#39;shapes of &quot;{}&quot; and &quot;{}&quot; not aligned: {} x {}&#39;.format(
                a, b, shape_a, shape_b))
        return (shape_a[0], shape_b[1])

    def _eval(self):
        a, b = self._ins
        return a.eval() @ b.eval()

    def propagate(self):
        a, b = self._ins
        a.send(self._dout @ b.eval().T)
        b.send(a.eval().T @ self._dout)
</code></pre>

<p>我们这里实现了一个简单的矩阵乘法，强制要求输入为矩阵，即<code>len(shape)==2</code>。后向传播所依据的公式为<br/>
\[C=AB \rightarrow \nabla_AL=\nabla_CL \cdot B^T, \quad \nabla_BL=A^T \cdot \nabla_CL.\]</p>

<p><code>propagate</code>中我们将梯度传给了所依赖的两个结点，<code>send</code>方法永远是在上游结点中调用的，上游结点送出梯度，这样看来起名为<code>send</code>还是有点道理的XD。</p>

<h4 id="toc_5">需要Broadcasting的运算</h4>

<p>接下来我们需要实现一些elementwise的运算如加减乘除，看似简单，但因为涉及到了broadcasting，还是需要想一想的。</p>

<p>所谓broadcasting，即两个不同形状的数组进行elementwise运算，在满足一定条件下，可以将“较小”的数组复制多份并拼接（即broadcast），使其形状一致再进行运算。这里我画张图给大家感受下：</p>

<p><img src="http://o6jlkx4pl.bkt.clouddn.com/15004655001108.jpg" alt=""/></p>

<p>按照<code>numpy</code>的<a href="https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">broadcasting rules</a>，两个数组的<code>shape</code>（即两个元组），从后向前逐位比较，如果1.相等或2.有一个为1，则算为兼容，可继续向前比较，直到有一个元组耗尽。</p>

<p>如果两个数组是兼容的，则可以进行broadcast：1.将低维度数组在高维度上进行broadcast；2.在比较时遇到1的维度上进行broadcast。例如：</p>

<pre><code>shape(2, 3, 5) + shape(5,) = shape(2, 3, 5)
shape(2, 3, 5) + shape(3, 5) = shape(2, 3, 5)
shape(2, 3, 5) + shape(2, 1, 5) = shape(2, 3, 5)
shape(2, 3, 5) + shape(3) -&gt; incompatible
shape(1, 5) + shape(5, 1) = shape(5, 5)
</code></pre>

<p>因为规则2的关系，会出现这种<code>shape(1, 5) + shape(5, 1) = shape(5, 5)</code>，处理起来比较麻烦，我们就暂不考虑了。</p>

<p>现在我们弄明白broadcasting的规则，就要想一下broadcast之后的运算，梯度怎么求。其实也很简单，因为broadcast实际上就是将原始数据复制多份，类似参数共享，那我们只需要将梯度沿之前broadcast过的维度加起来就好了。从数学的角度看这个问题，只考虑2维数组（矩阵）和1维数组（向量）的运算</p>

<p>\[<br/>
A \circ b^T = <br/>
    \begin{bmatrix}<br/>
        a_1^T \\<br/>
        a_2^T \\<br/>
        \cdots \\<br/>
        a_m^T<br/>
    \end{bmatrix} \circ b^T =<br/>
\begin{bmatrix}<br/>
        a_1^T \circ b^T\\<br/>
        a_2^T \circ b^T\\<br/>
        \cdots \\<br/>
        a_m^T \circ b^T<br/>
    \end{bmatrix} = A \circ (e b^T) = A\circ D,<br/>
\]<br/>
其中\(D = e b^T\)，\(e\)为全为1的列向量，\(\circ\)为某elementwise运算。这样，在我们得到关于\(D\)的梯度\(\nabla_D\)后，很容易算出关于\(b^T\)的梯度\(\nabla_{b^T} = e^T\nabla_D\)，即对矩阵\(\nabla_D\)进行逐列求和。</p>

<p><img src="http://o6jlkx4pl.bkt.clouddn.com/15004655223020.jpg" alt=""/></p>

<p>代码如下：</p>

<pre><code>class BroadcastMixin:
    &quot;&quot;&quot;a mixin class providing &#39;_infer_shape&#39; method&quot;&quot;&quot;

    def _infer_shape(self, a, b):
        &quot;&quot;&quot;Infer shape according to numpy broadcasting rule.&quot;&quot;&quot;
        shape_a = a.shape
        shape_b = b.shape
        rank_a = len(shape_a)
        rank_b = len(shape_b)
        self._sum_over_a = []  # axes along which a sum is performed
        self._sum_over_b = []
        if rank_a &gt; rank_b:
            ret_shape = list(shape_a)
            self._squeeze_over_a = ()  # axes that will be removed
            self._squeeze_over_b = tuple(range(rank_a - rank_b))
        else:
            ret_shape = list(shape_b)
            self._squeeze_over_a = tuple(range(rank_b - rank_a))
            self._squeeze_over_b = ()
        for d_a, d_b, k in zip(reversed(shape_a),
                               reversed(shape_b),
                               range(max(rank_a, rank_b)-1, -1, -1)):
            if d_a == d_b:  # != 1
                continue
            elif d_a == 1:
                ret_shape[k] = d_b
                self._sum_over_a.append(k)
            elif d_b == 1:
                ret_shape[k] = d_a
                self._sum_over_b.append(k)
            else:
                raise ValueError(&#39;operands could not be &#39;
                                 &#39;broadcast together with &#39;
                                 &#39;shapes {} {}&#39;.format(shape_a, shape_b))
        self._sum_over_a = self._squeeze_over_a + tuple(reversed(self._sum_over_a))
        self._sum_over_b = self._squeeze_over_b + tuple(reversed(self._sum_over_b))
        return tuple(ret_shape)

    def _sum_gradient_and_send(self, da, db):
        &quot;&quot;&quot;Sum the gradients and then send them.&quot;&quot;&quot;
        a, b = self._ins
        if self._sum_over_a:
            summed_da = da.sum(axis=self._sum_over_a, keepdims=True)
            summed_da = np.squeeze(summed_da, axis=self._squeeze_over_a)
        else:
            summed_da = da
        if self._sum_over_b:
            summed_db = db.sum(axis=self._sum_over_b, keepdims=True)
            summed_db = np.squeeze(summed_db, axis=self._squeeze_over_b)
        else:
            summed_db = db
        a.send(summed_da)
        b.send(summed_db)
</code></pre>

<p>这里我搞了一个混入类，不知道合不合适，主要是不想再继承<code>Op</code>了。这个类提供2个方法：<code>_infer_shape</code>推断形状，<code>_sum_gradient_and_send</code>对梯度求和并<code>send</code>给之前的结点。</p>

<p><code>_infer_shape</code>推断形状主要就是<code>if</code>那一块，但大部分代码都用来记录两组变量，<code>_sum_over_*</code>与<code>_squeeze_over_*</code>。接收到完整梯度（上文的\(\nabla_D\)）后，<code>_sum_over_*</code>告诉我们要对哪几个维求和，由于我们调用<code>sum</code>时令<code>keepdims=True</code>，这意味着求完和后，还要消除掉冗余的维，冗余的边由<code>_squeeze_over_*</code>提供。</p>

<p>举个例子：</p>

<blockquote>
<p>a.shape = (5, 1, 3)<br/>
b.shape = (2, 3)<br/>
c = a+b; c.shape = (5, 2, 3)<br/>
a的第1维会被broadcast，因此<code>sum_over_a=(1,)</code>，<code>squeeze_over_a=()</code><br/>
因为a的维数比b的高，b会新增一个第0维并broadcast，所以<code>squeeze_over_a=()</code>，<code>squeeze_over_b=(0,)</code><br/>
...<br/>
接收到梯度：da.shape=(5, 2, 3), db.shape=(5, 2, 3)<br/>
<code>summed_da = da.sum(axis=(1,), keepdims=True)</code><br/>
summed_da.shape=(5, 1, 3) 形状与a一致，收工<br/>
<code>summed_db = db.sum(axis=(0,), keepdims=True)</code><br/>
summed_db.shape=(1, 2, 3) 多了一维<br/>
<code>summed_db = np.squeeze(summed_db, axis=(0,))</code><br/>
summed_db.shape=(2, 3) 去掉第0维，形状与b一致，收工！</p>
</blockquote>

<p>这样，基础的四则运算代码就很简单啦，这里只放上加法的代码：</p>

<pre><code>class Add(BroadcastMixin, Op):

    def __init__(self, a, b, name=None):
        super().__init__(ins=[a, b], name=name)

    def _eval(self):
        a, b = self._ins
        return a.eval() + b.eval()

    def propagate(self):
        a, b = self._ins
        self._sum_gradient_and_send(self._dout, self._dout)
</code></pre>

<h4 id="toc_6">Reduce</h4>

<p><code>Reduce</code>类运算是一类非常关键的运算，如求和求均值取最大值等，不过实现也简单，就不多说了，这里放上求均值的代码。</p>

<pre><code>class ReduceMean(Op):

    def __init__(self, a, name=None):
        super().__init__(ins=[a], name=name)

    def _infer_shape(self, a):
        return ()

    def _eval(self):
        a = self._ins[0]
        return a.eval().mean()

    def propagate(self):
        a = self._ins[0]
        n = np.product(a.shape)
        a.send(np.full(a.shape, fill_value=self._dout / n, dtype=DEFAULTS[&#39;dtype&#39;]))
</code></pre>

<h4 id="toc_7">其他运算符</h4>

<p>幂的代码就写在这里了，没太多好说的，唯一需要注意的就是，我偷懒，没把p设置为一个Node，不过对于线性回归来说已经够了。类似的，指数运算也很好写，下次再写好了XD</p>

<pre><code>class Pow(Op):

    def __init__(self, a, p, name=None):
        &quot;&quot;&quot;
        Calculate a ** p.
        :param a: a Node instance
        :param p: a real number
        :param name: the name of Node
        &quot;&quot;&quot;
        super().__init__(ins=[a], name=name)
        self._p = p

    def _infer_shape(self, a):
        return a.shape

    def _eval(self):
        a = self._ins[0]
        return a.eval() ** self._p

    def propagate(self):
        a = self._ins[0]
        a.send(self._p * self._dout * a.eval() ** (self._p - 1))
</code></pre>

<h3 id="toc_8">重载运算符</h3>

<p>为了方便代码书写，我们回过头将<code>Node</code>类型的运算符重载了：</p>

<pre><code>class Node:
    def __matmul__(self, b):
        return Matmul(self, b)

    def __add__(self, b):
        return Add(self, b)

    def __sub__(self, b):
        return Subtract(self, b)

    def __rsub__(self, a):
        return Subtract(a, self)

    def __mul__(self, b):
        return Multiply(self, b)

    def __truediv__(self, b):
        return Divide(self, b)

    def __rtruediv__(self, a):
        return Divide(a, self)

    def __pow__(self, power, modulo=None):
        return Pow(self, power)
</code></pre>

<h2 id="toc_9">实战：线性回归</h2>

<p>到了实战环节，我们要用之前构建的代码，实现最简单的线性回归。<a href="https://github.com/YaoZh1918/PyNeuralNet/blob/master/pynn_v1_demo.ipynb">notebook</a>在这里。</p>

<h3 id="toc_10">生成数据</h3>

<pre><code>from matplotlib import pyplot as plt
import numpy as np
import neuralnet_v1 as pynn

# generate data
def batch_generator(batch_size, weight, bias):
    n = len(weight)
    while True:
        X = np.random.rand(batch_size, n)
        y = X @ weight + bias + np.random.rand(batch_size) * .01
        yield X, y

batch_size = 128
n = 50

w_true = np.random.randn(n, 1)
b_true = np.random.randn()
batch_iter = batch_generator(batch_size, w_true, b_true)
</code></pre>

<h3 id="toc_11">定义计算图</h3>

<p>类似tensorflow，<code>with graph.as_default():</code>之后，把定义写在代码块中就好。</p>

<pre><code># Define Graph

graph = pynn.Graph()
with graph.as_default():
# with pynn.Graph().as_default() as graph:  # alternative
    X_input = pynn.Input(shape=(batch_size, n), name=&#39;Input_X&#39;)
    y_input = pynn.Input(shape=(batch_size,), name=&#39;Input_y&#39;)
    W = pynn.Variable(np.random.randn(n, 1), name=&#39;Weights&#39;)
    b = pynn.Variable(np.random.randn(), name=&#39;Bias&#39;)
    y_pred = X_input @ W + b
    loss = pynn.ReduceMean(((y_pred - y_input) ** 2), name=&#39;loss&#39;)
</code></pre>

<p>定义好后可以可视化一下我们的计算图：</p>

<p><img src="http://o6jlkx4pl.bkt.clouddn.com/15004746401403.jpg" alt="Computation Graph"/></p>

<p>还不错，可惜<code>networkx</code>画图不够漂亮。</p>

<h3 id="toc_12">优化</h3>

<p>优化就用最简单的SGD，这里可以看到我们的<code>one_pass</code>的用法。</p>

<pre><code>alpha = .01
err = []
for i in range(5000):
    X_tr, y_tr = next(batch_iter)
    with graph.one_pass(feed_dict={X_input: X_tr, y_input: y_tr}):
        graph.forward()
        err.append(loss.eval())
        graph.backward()
        for node in graph.updatable_nodes:
            node._value -= alpha * node.gradient
</code></pre>

<p>最后画一下损失函数的图：</p>

<p><img src="http://o6jlkx4pl.bkt.clouddn.com/15004747430538.jpg" alt="Loss"/></p>

<p>还不错哟！</p>

<p>这期就到这里了，下次我们将计划实现卷积操作，并在MNIST数据集上进行实验。</p>


    

      </div>

      <div class="row">
        <div class="large-6 columns">
        <p class="text-left" style="padding:15px 0px;">
      
        </p>
        </div>
        <div class="large-6 columns">
      <p class="text-right" style="padding:15px 0px;">
      
          <a  href="15000325978554.html" 
          title="Next Post: Python实现人工神经网络(1)">Python实现人工神经网络(1) &raquo;</a>
      
      </p>
        </div>
      </div>
      <div class="comments-wrap">
        <div class="share-comments">
          

          

          
              <div class="ds-thread" data-thread-key="15004749480094.html" data-url="http://yaozh1918.github.io/15004749480094.html" data-title="Python实现人工神经网络(2)"></div>
          
        </div>
      </div>
    </div><!-- article-wrap -->
  </div><!-- large 8 -->




 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="asset/icon.jpg" /></div>
            
                <h1>YaoZh1918</h1>
                <div class="site-des">I'm DataMan!</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/YaoZh1918" title="GitHub">GitHub</a>

  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="miscellaneous.html"><strong>Miscellaneous</strong></a>
        
            <a href="algorithm.html"><strong>Algorithm</strong></a>
        
            <a href="python.html"><strong>Python</strong></a>
        
            <a href="linux.html"><strong>Linux</strong></a>
        
            <a href="Math.html"><strong>Math</strong></a>
        
            <a href="ml.html"><strong>Machine Learning</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15004749480094.html">Python实现人工神经网络(2)</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15000325978554.html">Python实现人工神经网络(1)</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14836189296241.html">利用Ngrok实现内网穿透</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14791104542390.html">A Smooth Thresholding Function</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14785830863216.html">Ubuntu上安装Transmission Web界面</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  


<script type="text/javascript">
var duoshuoQuery = {short_name:'yaozh1918'};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
